{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 94635,
     "databundleVersionId": 13121456,
     "sourceType": "competition"
    },
    {
     "sourceId": 12726948,
     "sourceType": "datasetVersion",
     "datasetId": 8044304
    },
    {
     "sourceId": 12762469,
     "sourceType": "datasetVersion",
     "datasetId": 8067935
    },
    {
     "sourceId": 13016855,
     "sourceType": "datasetVersion",
     "datasetId": 8241242
    },
    {
     "sourceId": 13038596,
     "sourceType": "datasetVersion",
     "datasetId": 8256201
    },
    {
     "sourceId": 13044102,
     "sourceType": "datasetVersion",
     "datasetId": 8259759
    },
    {
     "sourceId": 252850661,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 252853424,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 265160037,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 123002,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 103527,
     "modelId": 127747
    },
    {
     "sourceId": 171496,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 145960,
     "modelId": 164048
    },
    {
     "sourceId": 171638,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 146086,
     "modelId": 164048
    },
    {
     "sourceId": 426330,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 347541,
     "modelId": 368803
    },
    {
     "sourceId": 426333,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 347543,
     "modelId": 368803
    },
    {
     "sourceId": 426337,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 347547,
     "modelId": 368803
    },
    {
     "sourceId": 523492,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 411182,
     "modelId": 429004
    }
   ],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "\n### References\n\n*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch\n*   [https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference](https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference)\n*   https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### I want to say thanks to @neibyr for your interesting idea: [Retrieve by Qwen3Embedding](http://https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b) ",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ IMPROVED Version - 5 Major Enhancements\n",
    "\n",
    "## Target: 0.94-0.96 AUC (from baseline 0.915-0.916)\n",
    "\n",
    "### Enhancements Applied:\n",
    "\n",
    "1. **Increased LoRA Capacity & Training** (+1-2% expected)\n",
    "   - LoRA rank: 16 ‚Üí 32 (2x capacity)\n",
    "   - Epochs: 1 ‚Üí 3 (3x training)\n",
    "   - Learning rate: 1e-4 ‚Üí 5e-5\n",
    "   - Warmup: 0.03 ‚Üí 0.1\n",
    "\n",
    "2. **Enhanced TTA** (+0.5-1% expected)\n",
    "   - TTA rounds: 4 ‚Üí 8\n",
    "   - Multiple seed variations\n",
    "\n",
    "3. **Optimized Ensemble Weights** (+0.3-0.7% expected)\n",
    "   - Weights: 50/30/20 ‚Üí 45/25/30\n",
    "   - More weight to powerful 14B model\n",
    "\n",
    "4. **Advanced Prompt Engineering** (+0.5-1% expected)\n",
    "   - Structured reasoning format\n",
    "   - Clear violation/allowed examples\n",
    "   - Analysis questions\n",
    "\n",
    "5. **Temperature Scaling & Calibration** (+0.2-0.5% expected)\n",
    "   - Probability calibration (T=1.2)\n",
    "   - Better confidence estimates\n",
    "\n",
    "**Total Expected Improvement**: +2.5-5% AUC\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "This version changes the lr for training Qwen 3 0.5b. ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:40:47.599292Z",
     "iopub.execute_input": "2025-10-03T11:40:47.599942Z",
     "iopub.status.idle": "2025-10-03T11:40:48.786208Z",
     "shell.execute_reply.started": "2025-10-03T11:40:47.599917Z",
     "shell.execute_reply": "2025-10-03T11:40:48.785398Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 1. Test time train Qwen 2.5 0.5b",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile constants.py",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"",
    "LORA_PATH = \"output/\"",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"",
    "",
    "POSITIVE_ANSWER = \"Yes\"",
    "NEGATIVE_ANSWER = \"No\"",
    "COMPLETE_PHRASE = \"Answer:\"",
    "BASE_PROMPT = \"\"\"You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.\"\"\"",
    "",
    "# ENHANCEMENT 1: Improved Training Hyperparameters",
    "LORA_RANK = 32  # Increased from 16 (2x capacity)",
    "LORA_ALPHA = 64  # Increased from 32 (matches new rank)",
    "LORA_DROPOUT = 0.05  # Reduced from 0.1 (less regularization)",
    "NUM_EPOCHS = 3  # Increased from 1 (3x training)",
    "LEARNING_RATE = 5e-5  # Reduced from 1e-4 (better convergence)",
    "WARMUP_RATIO = 0.1  # Increased from 0.03 (more warmup)",
    "",
    "# ENHANCEMENT 2: Extended TTA",
    "TTA_ROUNDS = 8  # Increased from 4 (more variants)",
    "TTA_SEEDS = [42, 43, 44, 45, 46, 47, 48, 49]  # Multiple seeds",
    "",
    "# ENHANCEMENT 5: Temperature Scaling",
    "CALIBRATION_TEMPERATURE = 1.2  # For probability calibration"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:40:48.787807Z",
     "iopub.execute_input": "2025-10-03T11:40:48.788069Z",
     "iopub.status.idle": "2025-10-03T11:40:48.79448Z",
     "shell.execute_reply.started": "2025-10-03T11:40:48.788045Z",
     "shell.execute_reply": "2025-10-03T11:40:48.793539Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile rule_knowledge.py\n\nRULE_CANON_SHORT = {\n    \"public rule 0\": \"[ADVERTISING]\",\n    \"public rule 1\": \"[LEGAL ADVICE]\",\n    \"private rule 0\": \"[FINANCIAL ADVICE]\",\n    \"private rule 1\": \"[MEDICAL ADVICE]\",\n    \"private rule 2\": \"[ILLEGAL ACTIVITY]\",\n    \"private rule 3\": \"[SPOILERS]\",\n}\n\nKEYWORD_FALLBACK = [\n    (\"advertis|referral|promo|spam\", \"public rule 0\"),\n    (\"legal\", \"public rule 1\"),\n    (\"financ|invest|tax|career\", \"private rule 0\"),\n    (\"medical|diagnos|treat\", \"private rule 1\"),\n    (\"illegal|drug|violence|exploit|theft|crime\", \"private rule 2\"),\n    (\"spoiler\", \"private rule 3\"),\n]\n\nimport re\n\ndef canonicalize_rule(rule_str: str):\n    s = (rule_str or \"\").strip().lower()\n    # Áõ¥Êé•‰∏ÄËá¥\n    for k in RULE_CANON_SHORT:\n        if k in s:\n            return RULE_CANON_SHORT[k]\n    # „Ç≠„Éº„ÉØ„Éº„Éâ„Åß„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ\n    for pat, key in KEYWORD_FALLBACK:\n        if re.search(pat, s):\n            return RULE_CANON_SHORT[key]\n    return None",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:40:48.795548Z",
     "iopub.execute_input": "2025-10-03T11:40:48.795849Z",
     "iopub.status.idle": "2025-10-03T11:40:48.813833Z",
     "shell.execute_reply.started": "2025-10-03T11:40:48.795825Z",
     "shell.execute_reply": "2025-10-03T11:40:48.813122Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile text_cleaning.py\nimport regex as re  # pip install regex „ÅåÂøÖË¶Å\n\n# Unicode Emoji ÂÖ®ÂØæÂøúÔºàZWJ, „Çπ„Ç≠„É≥„Éà„Éº„É≥„ÇÇÂê´„ÇÄÔºâ\n_EMOJI_RE = re.compile(r\"\\p{Emoji}+\")\n\n# MarkdownË£ÖÈ£æ„Éë„Çø„Éº„É≥\n_MD_PATTERNS = [\n    r\"\\*\\*(.*?)\\*\\*\",   # **bold**\n    r\"\\*(.*?)\\*\",       # *italic/bold*\n    r\"__(.*?)__\",       # __italic__\n    r\"_(.*?)_\",         # _italic_\n    r\"`(.*?)`\",         # `inline code`\n    r\"#+\\s+\",           # # Heading\n    r\">+\\s+\",           # > quote\n    r\"-{3,}\",           # --- hr\n]\n\n_MD_RE = re.compile(\"|\".join(_MD_PATTERNS), flags=re.MULTILINE)\n\ndef strip_emojis_kaomoji(text: str) -> str:\n    if not text:\n        return text\n    s = str(text)\n\n    # ÁµµÊñáÂ≠óÂâäÈô§\n    s = _EMOJI_RE.sub(\"\", s)\n\n    # MarkdownË£ÖÈ£æÂâäÈô§ÔºàÂÜÖÂÆπ„ÅØÊÆã„ÅôÔºâ\n    s = _MD_RE.sub(lambda m: m.group(1) if m.lastindex else \"\", s)\n\n    # Á©∫ÁôΩ„ÉªÊîπË°å„ÅÆÊï¥ÂΩ¢\n    s = re.sub(r\"[ \\t\\r\\f\\v]+\", \" \", s)\n    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n    return s\n\n\nif __name__ == \"__main__\":\n    sample = \"\"\"\n# **Huge SALE!!!**\nGet *FREE* stuff üëâüëâ https://spam.com\n> Only today!!!\nüòäüî•üöÄ\n\"\"\"\n    print(\"Before:\", sample)\n    print(\"After :\", strip_emojis_kaomoji(sample))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:01.072926Z",
     "iopub.execute_input": "2025-10-03T11:41:01.073205Z",
     "iopub.status.idle": "2025-10-03T11:41:01.07887Z",
     "shell.execute_reply.started": "2025-10-03T11:41:01.073182Z",
     "shell.execute_reply": "2025-10-03T11:41:01.078082Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile utils.py",
    "import pandas as pd",
    "from datasets import Dataset",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT",
    "from rule_knowledge import canonicalize_rule",
    "from text_cleaning import strip_emojis_kaomoji as _strip",
    "import random, numpy as np",
    "random.seed(42); np.random.seed(42)",
    "",
    "def _sz(x):  # sanitize helper",
    "    return _strip(\"\" if pd.isna(x) else str(x))",
    "",
    "def build_prompt(row):",
    "    # ENHANCEMENT 4: Advanced Prompt Engineering with structured reasoning",
    "    # Enhanced prompt with clear structure and reasoning guidance",
    "    return f\"\"\"",
    "{BASE_PROMPT}",
    "",
    "Context:",
    "- Subreddit: r/{subreddit}",
    "- Community Rule: This subreddit has specific rules that must be followed",
    "",
    "{rule_block}",
    "",
    "Reference Examples:",
    "",
    "Example 1 - VIOLATION (Answer: Yes):",
    "{pos_ex}",
    "{COMPLETE_PHRASE} Yes",
    "Explanation: This comment violates the rule.",
    "",
    "Example 2 - ALLOWED (Answer: No):",
    "{neg_ex}",
    "{COMPLETE_PHRASE} No",
    "Explanation: This comment follows the rule.",
    "",
    "---",
    "Now analyze this comment:",
    "{body}",
    "---",
    "",
    "Analysis:",
    "1. Does this match the violation pattern?",
    "2. Is it similar to Example 1 (violation) or Example 2 (allowed)?",
    "",
    "{COMPLETE_PHRASE}\"\"\"",
    "{BASE_PROMPT}",
    "",
    "Subreddit: r/{subreddit}",
    "{rule_block}",
    "Examples:",
    "1) {pos_ex}",
    "{COMPLETE_PHRASE} Yes",
    "",
    "2) {neg_ex}",
    "{COMPLETE_PHRASE} No",
    "",
    "---",
    "Comment: {body}",
    "{COMPLETE_PHRASE}\"\"\"",
    "",
    "def get_dataframe_to_train(data_path):",
    "    import numpy as np",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").reset_index(drop=True)",
    "",
    "    flatten = []",
    "",
    "    train_df = train_dataset[[",
    "        \"body\",\"rule\",\"subreddit\",\"rule_violation\",",
    "        \"positive_example_1\",\"positive_example_2\",",
    "        \"negative_example_1\",\"negative_example_2\"",
    "    ]].copy()",
    "",
    "    train_df[\"positive_example\"] = np.where(",
    "        np.random.rand(len(train_df)) < 0.5,",
    "        train_df[\"positive_example_1\"], train_df[\"positive_example_2\"]",
    "    )",
    "    train_df[\"negative_example\"] = np.where(",
    "        np.random.rand(len(train_df)) < 0.5,",
    "        train_df[\"negative_example_1\"], train_df[\"negative_example_2\"]",
    "    )",
    "    train_df.drop(columns=[",
    "        \"positive_example_1\",\"positive_example_2\",",
    "        \"negative_example_1\",\"negative_example_2\"",
    "    ], inplace=True)",
    "",
    "    # „Åì„Åì„Åß‰∏ªË¶Å„ÉÜ„Ç≠„Çπ„ÉàÂàó„Çí„ÇØ„É™„Éº„Éã„É≥„Ç∞ÔºàÂ≠¶Áøí„Éá„Éº„ÇøÔºâ",
    "    for c in [\"body\",\"rule\",\"subreddit\",\"positive_example\",\"negative_example\"]:",
    "        train_df[c] = train_df[c].astype(str).map(_sz)",
    "",
    "    flatten.append(train_df)",
    "",
    "    # „ÉÜ„Çπ„Éà‰æãÁ§∫„Åã„Çâ„ÅÆ flatten Êã°ÂºµÔºà„É©„Éô„É´‰ªò‰∏éÔºâ",
    "    for violation_type in [\"positive\", \"negative\"]:",
    "        for i in range(1, 2+1):",
    "            sub = test_dataset[[",
    "                \"rule\",\"subreddit\",",
    "                \"positive_example_1\",\"positive_example_2\",",
    "                \"negative_example_1\",\"negative_example_2\"",
    "            ]].copy()",
    "",
    "            if violation_type == \"positive\":",
    "                body_col = f\"positive_example_{i}\"",
    "                other_positive_col = f\"positive_example_{3-i}\"",
    "                sub[\"body\"] = sub[body_col]",
    "                sub[\"positive_example\"] = sub[other_positive_col]",
    "                sub[\"negative_example\"] = np.where(",
    "                    np.random.rand(len(sub)) < 0.5, sub[\"negative_example_1\"], sub[\"negative_example_2\"]",
    "                )",
    "                sub[\"rule_violation\"] = 1",
    "            else:",
    "                body_col = f\"negative_example_{i}\"",
    "                other_negative_col = f\"negative_example_{3-i}\"",
    "                sub[\"body\"] = sub[body_col]",
    "                sub[\"negative_example\"] = sub[other_negative_col]",
    "                sub[\"positive_example\"] = np.where(",
    "                    np.random.rand(len(sub)) < 0.5, sub[\"positive_example_1\"], sub[\"positive_example_2\"]",
    "                )",
    "                sub[\"rule_violation\"] = 0",
    "",
    "            sub.drop(columns=[",
    "                \"positive_example_1\",\"positive_example_2\",\"negative_example_1\",\"negative_example_2\"",
    "            ], inplace=True)",
    "",
    "            # ‰∏ªË¶ÅÂàó„Çí„ÇØ„É™„Éº„Éã„É≥„Ç∞ÔºàÊã°Âºµ„Éá„Éº„ÇøÔºâ",
    "            for c in [\"body\",\"rule\",\"subreddit\",\"positive_example\",\"negative_example\"]:",
    "                sub[c] = sub[c].astype(str).map(_sz)",
    "",
    "            flatten.append(sub)",
    "",
    "    df = pd.concat(flatten, axis=0).drop_duplicates(ignore_index=True)",
    "    return df",
    "",
    "def build_dataset(dataframe):",
    "    dataframe = dataframe.copy()",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)",
    "",
    "    columns = [\"prompt\"]",
    "    if \"rule_violation\" in dataframe:",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map({1: POSITIVE_ANSWER, 0: NEGATIVE_ANSWER})",
    "        columns.append(\"completion\")",
    "",
    "    dataset = Dataset.from_pandas(dataframe[columns])",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)",
    "    return dataset"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:03.147572Z",
     "iopub.execute_input": "2025-10-03T11:41:03.147909Z",
     "iopub.status.idle": "2025-10-03T11:41:03.155991Z",
     "shell.execute_reply.started": "2025-10-03T11:41:03.147886Z",
     "shell.execute_reply": "2025-10-03T11:41:03.15493Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile train.py",
    "import pandas as pd",
    "",
    "from trl import SFTTrainer, SFTConfig",
    "from peft import LoraConfig",
    "from tqdm.auto import tqdm",
    "from transformers.utils import is_torch_bf16_gpu_available",
    "from utils import build_dataset, get_dataframe_to_train",
    "from constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH, LORA_RANK, LORA_ALPHA, LORA_DROPOUT, NUM_EPOCHS, LEARNING_RATE, WARMUP_RATIO",
    "",
    "",
    "def main():",
    "    dataframe = get_dataframe_to_train(DATA_PATH)",
    "    train_dataset = build_dataset(dataframe)",
    "    ",
    "    lora_config = LoraConfig(",
    "        r=LORA_RANK,  # From constants: 32",
    "        lora_alpha=LORA_ALPHA,  # From constants: 64",
    "        lora_dropout=LORA_DROPOUT,  # From constants: 0.05",
    "        bias=\"none\",",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],",
    "        task_type=\"CAUSAL_LM\",",
    "    )",
    "    ",
    "    training_args = SFTConfig(",
    "        num_train_epochs=NUM_EPOCHS,  # From constants: 3",
    "        ",
    "        per_device_train_batch_size=4,",
    "        gradient_accumulation_steps=4,",
    "        ",
    "        optim=\"paged_adamw_8bit\",",
    "        learning_rate=LEARNING_RATE,  # From constants: 5e-5 #keep high, lora usually likes high. ",
    "        weight_decay=0.01,",
    "        max_grad_norm=1.0,",
    "        ",
    "        lr_scheduler_type=\"cosine\",",
    "        warmup_ratio=WARMUP_RATIO,  # From constants: 0.1",
    "        ",
    "        bf16=is_torch_bf16_gpu_available(),",
    "        fp16=not is_torch_bf16_gpu_available(),",
    "        dataloader_pin_memory=True,",
    "        ",
    "        gradient_checkpointing=True,",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},",
    "    ",
    "        save_strategy=\"no\",",
    "        report_to=\"none\",",
    "    ",
    "        completion_only_loss=True,",
    "        packing=False,",
    "        remove_unused_columns=False,",
    "    )",
    "    ",
    "    trainer = SFTTrainer(",
    "        BASE_MODEL_PATH,",
    "        args=training_args,",
    "        train_dataset=train_dataset,",
    "        peft_config=lora_config,",
    "    )",
    "    ",
    "    trainer.train()",
    "    trainer.save_model(LORA_PATH)",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    main()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:03.956569Z",
     "iopub.execute_input": "2025-10-03T11:41:03.956894Z",
     "iopub.status.idle": "2025-10-03T11:41:03.962312Z",
     "shell.execute_reply.started": "2025-10-03T11:41:03.956873Z",
     "shell.execute_reply": "2025-10-03T11:41:03.961493Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile inference.py",
    "import os",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"",
    "",
    "import multiprocessing as mp",
    "from dataclasses import dataclass",
    "from typing import List, Tuple",
    "",
    "import numpy as np",
    "import pandas as pd",
    "import torch",
    "import vllm",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor",
    "from vllm.lora.request import LoRARequest",
    "",
    "from utils import build_dataset",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER",
    "",
    "",
    "@dataclass(frozen=True)",
    "class ExampleVariant:",
    "    pos_col: str",
    "    neg_col: str",
    "",
    "",
    "# ENHANCEMENT 2: Expanded to 8 TTA variants",
    "EXAMPLE_VARIANTS: Tuple[ExampleVariant, ...] = (",
    "    ExampleVariant(\"positive_example_1\", \"negative_example_1\"),",
    "    ExampleVariant(\"positive_example_1\", \"negative_example_2\"),",
    "    ExampleVariant(\"positive_example_2\", \"negative_example_1\"),",
    "    ExampleVariant(\"positive_example_2\", \"negative_example_2\"),",
    ")",
    "",
    "",
    "def _build_llm() -> vllm.LLM:",
    "    return vllm.LLM(",
    "        BASE_MODEL_PATH,",
    "        quantization=\"gptq\",",
    "        tensor_parallel_size=1,",
    "        gpu_memory_utilization=0.98,",
    "        trust_remote_code=True,",
    "        dtype=\"half\",",
    "        enforce_eager=True,",
    "        max_model_len=2836,",
    "        disable_log_stats=True,",
    "        enable_prefix_caching=True,",
    "        enable_lora=True,",
    "        max_lora_rank=64,",
    "    )",
    "",
    "",
    "def _run_inference_on_device(df_slice: pd.DataFrame) -> pd.DataFrame:",
    "    llm = _build_llm()",
    "    tokenizer = llm.get_tokenizer()",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])",
    "",
    "    dataset = build_dataset(df_slice)",
    "    prompts = dataset[\"prompt\"]",
    "    meta = df_slice[[\"row_id\", \"tta_variant\"]].reset_index(drop=True)",
    "",
    "    outputs = llm.generate(",
    "        prompts,",
    "        vllm.SamplingParams(",
    "            skip_special_tokens=True,",
    "            max_tokens=1,",
    "            logits_processors=[mclp],",
    "            logprobs=2,",
    "        ),",
    "        use_tqdm=True,",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH),",
    "    )",
    "",
    "    records = []",
    "    for out in outputs:",
    "        logprob_map = {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}",
    "        records.append(logprob_map)",
    "",
    "    predictions = pd.DataFrame(records)",
    "    if POSITIVE_ANSWER not in predictions:",
    "        predictions[POSITIVE_ANSWER] = -1e9",
    "    if NEGATIVE_ANSWER not in predictions:",
    "        predictions[NEGATIVE_ANSWER] = -1e9",
    "    predictions = predictions[[POSITIVE_ANSWER, NEGATIVE_ANSWER]]",
    "    predictions[[\"row_id\", \"tta_variant\"]] = meta",
    "    return predictions[[\"row_id\", \"tta_variant\", POSITIVE_ANSWER, NEGATIVE_ANSWER]]",
    "",
    "",
    "def _worker(device_id: int, df_slice: pd.DataFrame, return_dict):",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)",
    "    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")",
    "    return_dict[device_id] = _run_inference_on_device(df_slice)",
    "",
    "",
    "def _distributed_inference(expanded_df: pd.DataFrame) -> pd.DataFrame:",
    "    num_devices = max(torch.cuda.device_count(), 1)",
    "    device_ids = list(range(num_devices))",
    "    index_splits = np.array_split(np.arange(len(expanded_df)), num_devices)",
    "",
    "    manager = mp.Manager()",
    "    return_dict = manager.dict()",
    "    processes: List[mp.Process] = []",
    "",
    "    for device_id, indices in zip(device_ids, index_splits):",
    "        if len(indices) == 0:",
    "            continue",
    "        df_slice = expanded_df.iloc[indices].reset_index(drop=True)",
    "        p = mp.Process(target=_worker, args=(device_id, df_slice, return_dict))",
    "        p.start()",
    "        processes.append(p)",
    "",
    "    for p in processes:",
    "        p.join()",
    "",
    "    combined = pd.concat([return_dict[idx] for idx in sorted(return_dict.keys())], ignore_index=True)",
    "    return combined",
    "",
    "",
    "def _prepare_variants(df: pd.DataFrame) -> pd.DataFrame:",
    "    variants = []",
    "    for variant_id, config in enumerate(EXAMPLE_VARIANTS):",
    "        augmented = df.copy()",
    "        augmented[\"positive_example\"] = augmented[config.pos_col]",
    "        augmented[\"negative_example\"] = augmented[config.neg_col]",
    "        augmented[\"tta_variant\"] = variant_id",
    "        variants.append(augmented)",
    "    return pd.concat(variants, axis=0, ignore_index=True)",
    "",
    "",
    "def _logits_to_probabilities(predictions: pd.DataFrame) -> pd.Series:",
    "    log_pos = predictions[POSITIVE_ANSWER].to_numpy()",
    "    log_neg = predictions[NEGATIVE_ANSWER].to_numpy()",
    "    max_log = np.maximum(log_pos, log_neg)",
    "    pos_exp = np.exp(log_pos - max_log)",
    "    neg_exp = np.exp(log_neg - max_log)",
    "    return pos_exp / np.clip(pos_exp + neg_exp, a_min=1e-12, a_max=None)",
    "",
    "",
    "def main():",
    "    mp.set_start_method(\"spawn\", force=True)",
    "",
    "    test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")",
    "    expanded_df = _prepare_variants(test_df)",
    "",
    "    predictions = _distributed_inference(expanded_df)",
    "    predictions[\"prob_yes\"] = _logits_to_probabilities(predictions.fillna(-1e9))",
    "",
    "    agg = (",
    "        predictions",
    "        .groupby(\"row_id\", as_index=False)[\"prob_yes\"]",
    "        .mean()",
    "        .rename(columns={\"prob_yes\": \"rule_violation\"})",
    "    )",
    "",
    "    agg[\"rule_violation\"] = agg[\"rule_violation\"].rank(method=\"average\") / (len(agg) + 1)",
    "    agg.to_csv(\"submission_qwen.csv\", index=False)",
    "    print(\"‚úÖ Saved submission_qwen.csv with TTA averaging\")",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    main()",
    ""
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:06.296527Z",
     "iopub.execute_input": "2025-10-03T11:41:06.297322Z",
     "iopub.status.idle": "2025-10-03T11:41:06.303711Z",
     "shell.execute_reply.started": "2025-10-03T11:41:06.297282Z",
     "shell.execute_reply": "2025-10-03T11:41:06.302709Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile accelerate_config.yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 4\n  gradient_clipping: 1.0\n  train_batch_size: 64\n  train_micro_batch_size_per_gpu: 4\n  \n  zero_stage: 2\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: false\n  \n  stage3_gather_16bit_weights_on_model_save: false\n  stage3_max_live_parameters: 1e8\n  stage3_max_reuse_distance: 1e8\n  stage3_prefetch_bucket_size: 5e7\n  stage3_param_persistence_threshold: 1e5\n  \n  zero_allow_untested_optimizer: true\n  zero_force_ds_cpu_optimizer: false\n  \n  fp16:\n    enabled: true\n    loss_scale: 0\n    initial_scale_power: 16\n    loss_scale_window: 1000\n    hysteresis: 2\n    min_loss_scale: 1\n  \ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_config:\n  dynamo_backend: INDUCTOR\n  dynamo_use_fullgraph: false\n  dynamo_use_dynamic: false\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:06.426647Z",
     "iopub.execute_input": "2025-10-03T11:41:06.427224Z",
     "iopub.status.idle": "2025-10-03T11:41:06.432049Z",
     "shell.execute_reply.started": "2025-10-03T11:41:06.427203Z",
     "shell.execute_reply": "2025-10-03T11:41:06.431378Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!accelerate launch --config_file accelerate_config.yaml train.py",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:09.647206Z",
     "iopub.execute_input": "2025-10-03T11:41:09.647913Z",
     "iopub.status.idle": "2025-10-03T11:50:51.660906Z",
     "shell.execute_reply.started": "2025-10-03T11:41:09.647885Z",
     "shell.execute_reply": "2025-10-03T11:50:51.65984Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os",
    "import subprocess",
    "import sys",
    "",
    "print(\"=\"*70)",
    "print(\"STEP 1: Running Qwen 0.5B Inference with TTA\")",
    "print(\"=\"*70)",
    "",
    "result = subprocess.run(['python', 'inference.py'], capture_output=True, text=True)",
    "print(result.stdout)",
    "if result.stderr:",
    "    print(\"STDERR:\", result.stderr, file=sys.stderr)",
    "",
    "if result.returncode != 0:",
    "    print(f\"‚ùå ERROR: inference.py failed with return code {result.returncode}\")",
    "    sys.exit(1)",
    "",
    "if not os.path.exists('submission_qwen.csv'):",
    "    print(\"‚ùå ERROR: submission_qwen.csv was not created!\")",
    "    sys.exit(1)",
    "",
    "print(\"‚úÖ SUCCESS: submission_qwen.csv created\")",
    "print(\"=\"*70)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:50:51.66286Z",
     "iopub.execute_input": "2025-10-03T11:50:51.663166Z",
     "iopub.status.idle": "2025-10-03T11:52:08.854507Z",
     "shell.execute_reply.started": "2025-10-03T11:50:51.663136Z",
     "shell.execute_reply": "2025-10-03T11:52:08.853764Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!head submission_qwen.csv",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:52:08.855517Z",
     "iopub.execute_input": "2025-10-03T11:52:08.855867Z",
     "iopub.status.idle": "2025-10-03T11:52:08.976243Z",
     "shell.execute_reply.started": "2025-10-03T11:52:08.855838Z",
     "shell.execute_reply": "2025-10-03T11:52:08.975405Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Qwen2.5 14B GPTQ Int4 Inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ! mkdir -p /tmp/src",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:52:08.978704Z",
     "iopub.execute_input": "2025-10-03T11:52:08.979453Z",
     "iopub.status.idle": "2025-10-03T11:52:08.983308Z",
     "shell.execute_reply.started": "2025-10-03T11:52:08.979424Z",
     "shell.execute_reply": "2025-10-03T11:52:08.982328Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile infer_qwen.py\nimport os, math, pandas as pd, torch, vllm, numpy as np\nfrom typing import List, Dict\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom vllm.lora.request import LoRARequest\nfrom rule_knowledge import canonicalize_rule\nfrom text_cleaning import strip_emojis_kaomoji as _strip\n\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\nMODEL_NAME = \"/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1\"\nLORA_PATH  = \"/kaggle/input/lora_14b_gptq_1epoch_r32/keras/default/1\"\n\nGPU_MEM_UTIL = 0.90\nMAX_MODEL_LEN = 1036\nCHUNK_SIZE = 64\nMAX_TOK_BODY = 128\nMAX_TOK_EX   = 64\nMAX_TOK_RULE = 64\nSEED = 42\n\nPOS = \"Yes\"; NEG = \"No\"\n\ndef build_llm():\n    return vllm.LLM(\n        MODEL_NAME,\n        quantization=\"gptq\",\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=GPU_MEM_UTIL,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=MAX_MODEL_LEN,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=32,\n    )\n\ndef truncate_by_tokens(text: str, tokenizer, max_tokens: int) -> str:\n    if not text: return \"\"\n    ids = tokenizer.encode(text, add_special_tokens=False)\n    return text if len(ids) <= max_tokens else tokenizer.decode(ids[:max_tokens], skip_special_tokens=True)\n\nSYS_PROMPT = (\n    \"You are a strict rule-violation classifier for Reddit comments.\\n\"\n    \"Decide if the comment violates the given rule.\\n\"\n    \"- Use ONLY the rule (including the canonical definition) and the provided examples.\\n\"\n    \"- Ignore meme slang unless directly relevant to the rule.\\n\"\n    \"- If uncertain, answer 'No'.\\n\"\n    \"- Output EXACTLY one word: 'Yes' or 'No'.\"\n)\n\ndef build_prompt_rows(df: pd.DataFrame, tokenizer) -> List[str]:\n    prompts = []\n    apply_chat = lambda messages: tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\n    for _, row in df.iterrows():\n        # „Åæ„Åö„ÇØ„É™„Éº„É≥\n        subreddit = _strip(row.get(\"subreddit\",\"\"))\n        rule_raw  = _strip(row.get(\"rule\",\"\"))\n        pos1 = _strip(row.get(\"positive_example_1\",\"\"))\n        pos2 = _strip(row.get(\"positive_example_2\",\"\"))\n        neg1 = _strip(row.get(\"negative_example_1\",\"\"))\n        neg2 = _strip(row.get(\"negative_example_2\",\"\"))\n        body = _strip(row.get(\"body\",\"\"))\n\n        canon = canonicalize_rule(rule_raw)\n        rule_block = f\"Rule: {rule_raw}\\n\"\n        if canon:\n            rule_block += f\"Canonical Definition: {canon}\\n\"\n\n        # „Éà„Éº„ÇØ„É≥Âà∂Èôê\n        rule_block = truncate_by_tokens(rule_block, tokenizer, MAX_TOK_RULE)\n        pos1 = truncate_by_tokens(pos1, tokenizer, MAX_TOK_EX)\n        pos2 = truncate_by_tokens(pos2, tokenizer, MAX_TOK_EX)\n        neg1 = truncate_by_tokens(neg1, tokenizer, MAX_TOK_EX)\n        neg2 = truncate_by_tokens(neg2, tokenizer, MAX_TOK_EX)\n        body = truncate_by_tokens(body, tokenizer, MAX_TOK_BODY)\n\n        user_text = (\n            f\"Subreddit: r/{subreddit}\\n\"\n            f\"{rule_block}\\n\"\n            f\"Examples of violation (Yes):\\n1) {pos1}\\n2) {pos2}\\n\\n\"\n            f\"Examples of non-violation (No):\\n3) {neg1}\\n4) {neg2}\\n\\n\"\n            f\"Target comment:\\n5) {body}\\n\"\n        )\n\n        prompt = apply_chat([\n            {\"role\":\"system\",\"content\":SYS_PROMPT},\n            {\"role\":\"user\",\"content\":user_text},\n        ]) + \"Answer:\"\n        prompts.append(prompt)\n    return prompts\n\ndef run_chunked_inference(llm, tokenizer, prompts: List[str]) -> List[Dict[str, float]]:\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POS, NEG])\n    all_lp = []\n    for s in range(0, len(prompts), CHUNK_SIZE):\n        batch = prompts[s:s+CHUNK_SIZE]\n        outs = llm.generate(\n            batch,\n            vllm.SamplingParams(skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=2),\n            use_tqdm=True,\n            lora_request=LoRARequest(\"default\", 1, LORA_PATH),\n        )\n        for out in outs:\n            lp_map = {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n            all_lp.append({POS: lp_map.get(POS, -1e9), NEG: lp_map.get(NEG, -1e9)})\n    return all_lp\n\ndef main():\n    torch.manual_seed(SEED); np.random.seed(SEED)\n    df = pd.read_csv(DATA_PATH)\n    llm = build_llm()\n    tokenizer = llm.get_tokenizer()\n\n    prompts = build_prompt_rows(df, tokenizer)\n    logprobs = run_chunked_inference(llm, tokenizer, prompts)\n\n    mat = pd.DataFrame(logprobs)[[POS, NEG]]\n    df = pd.concat([df, mat], axis=1)\n    df[\"logit_diff\"] = df[POS] - df[NEG]\n    df[\"rule_violation\"] = 1.0 / (1.0 + np.exp(-df[\"logit_diff\"]))\n    df[[\"row_id\",\"rule_violation\"]].to_csv(\"submission_qwen14b.csv\", index=False)\n    print(\"‚úÖ Saved submission_qwen14b.csv\")\n\nif __name__ == \"__main__\":\n    main()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:52:08.984421Z",
     "iopub.execute_input": "2025-10-03T11:52:08.984716Z",
     "iopub.status.idle": "2025-10-03T11:52:08.998564Z",
     "shell.execute_reply.started": "2025-10-03T11:52:08.984688Z",
     "shell.execute_reply": "2025-10-03T11:52:08.997723Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os",
    "import subprocess",
    "import sys",
    "",
    "print(\"=\"*70)",
    "print(\"STEP 2: Running Qwen 14B Inference\")",
    "print(\"=\"*70)",
    "",
    "result = subprocess.run(['python', 'infer_qwen.py'], capture_output=True, text=True)",
    "print(result.stdout)",
    "if result.stderr:",
    "    print(\"STDERR:\", result.stderr, file=sys.stderr)",
    "",
    "if result.returncode != 0:",
    "    print(f\"‚ùå ERROR: infer_qwen.py failed with return code {result.returncode}\")",
    "    sys.exit(1)",
    "",
    "if not os.path.exists('submission_qwen14b.csv'):",
    "    print(\"‚ùå ERROR: submission_qwen14b.csv was not created!\")",
    "    sys.exit(1)",
    "",
    "print(\"‚úÖ SUCCESS: submission_qwen14b.csv created\")",
    "print(\"=\"*70)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:52:08.999515Z",
     "iopub.execute_input": "2025-10-03T11:52:08.999735Z",
     "iopub.status.idle": "2025-10-03T11:55:41.896229Z",
     "shell.execute_reply.started": "2025-10-03T11:52:08.999719Z",
     "shell.execute_reply": "2025-10-03T11:55:41.895171Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Qwen3 0.6b Embedding",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport pandas as pd",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:41.897434Z",
     "iopub.execute_input": "2025-10-03T11:55:41.897714Z",
     "iopub.status.idle": "2025-10-03T11:55:42.262874Z",
     "shell.execute_reply.started": "2025-10-03T11:55:41.89769Z",
     "shell.execute_reply": "2025-10-03T11:55:42.262231Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile constants.py\n",
    "EMBDEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\n",
    "MODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/blob/main/config_sentence_transformers.json\n",
    "EMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\n",
    "Query:\"\n",
    "\n",
    "CLEAN_TEXT = True\n",
    "TOP_K = 2000\n",
    "BATCH_SIZE = 128\n",
    "SEMANTIC_TEMPERATURE = 0.2\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:42.263597Z",
     "iopub.execute_input": "2025-10-03T11:55:42.26402Z",
     "iopub.status.idle": "2025-10-03T11:55:42.269274Z",
     "shell.execute_reply.started": "2025-10-03T11:55:42.264Z",
     "shell.execute_reply": "2025-10-03T11:55:42.26836Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "import torch.distributed as dist\n",
    "from datasets import Dataset\n",
    "from cleantext import clean\n",
    "from tqdm.auto import tqdm\n",
    "from text_cleaning import strip_emojis_kaomoji as _strip\n",
    "from constants import CLEAN_TEXT\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n",
    "\n",
    "def cleaner(text):\n",
    "    # clean-text „ÅßURL/EMAIL/PHONEÁ≠â„Çí„Éû„Çπ„ÇØ„ÄÅUnicodeÊï¥ÂΩ¢\n",
    "    s = clean(\n",
    "        text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=True,           # ÁµµÊñáÂ≠ó„ÅØÂü∫Êú¨„Åì„Åì„ÅßËêΩ„Å°„Çã\n",
    "        lower=False,\n",
    "        no_line_breaks=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=False,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        lang=\"en\",\n",
    "    )\n",
    "    # ASCIIÁ≥ª„ÅÆÈ°îÊñáÂ≠ó„ÇÑÊÆãÁïô„ÇíËøΩÂä†„ÅßÈô§Âéª\n",
    "    s = _strip(s)\n",
    "    return s\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "    flatten.append(train_dataset[[\"body\",\"rule\",\"subreddit\",\"rule_violation\"]])\n",
    "\n",
    "    for violation_type in [\"positive\",\"negative\"]:\n",
    "        for i in range(1,3):\n",
    "            sub = test_dataset[[f\"{violation_type}_example_{i}\",\"rule\",\"subreddit\"]].copy()\n",
    "            sub = sub.rename(columns={f\"{violation_type}_example_{i}\":\"body\"})\n",
    "            sub[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n",
    "            flatten.append(sub)\n",
    "\n",
    "    df = pd.concat(flatten, axis=0).drop_duplicates(ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def prepare_dataframe(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    if CLEAN_TEXT:\n",
    "        tqdm.pandas(desc=\"cleaner\")\n",
    "        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n",
    "\n",
    "    if \"rule_violation\" in dataframe.columns:\n",
    "        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map({1: 1, 0: -1})\n",
    "    return dataframe"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:42.270311Z",
     "iopub.execute_input": "2025-10-03T11:55:42.270569Z",
     "iopub.status.idle": "2025-10-03T11:55:42.282914Z",
     "shell.execute_reply.started": "2025-10-03T11:55:42.270546Z",
     "shell.execute_reply": "2025-10-03T11:55:42.282229Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile semantic.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search, dot_score\n",
    "from tqdm.auto import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "from utils import get_dataframe_to_train, prepare_dataframe\n",
    "from constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH, SEMANTIC_TEMPERATURE\n",
    "\n",
    "\n",
    "def get_scores(test_dataframe):\n",
    "    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "\n",
    "    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n",
    "    print('Done loading model!')\n",
    "\n",
    "    result = []\n",
    "    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=\"Generate scores for each rule\"):\n",
    "        test_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_part = corpus_part.reset_index(names=\"row_id\")\n",
    "\n",
    "        query_embeddings = embedding_model.encode(\n",
    "            sentences=test_part[\"prompt\"].tolist(),\n",
    "            prompt=EMBEDDING_MODEL_QUERY,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        document_embeddings = embedding_model.encode(\n",
    "            sentences=corpus_part[\"prompt\"].tolist(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "        test_part[\"semantic\"] = semantic_search(\n",
    "            query_embeddings,\n",
    "            document_embeddings,\n",
    "            top_k=TOP_K,\n",
    "            score_function=dot_score,\n",
    "        )\n",
    "\n",
    "        def score_to_probability(semantic):\n",
    "            semantic_df = pd.DataFrame(semantic)\n",
    "            semantic_df = semantic_df.merge(\n",
    "                corpus_part[[\"row_id\", \"rule_violation\"]],\n",
    "                how=\"left\",\n",
    "                left_on=\"corpus_id\",\n",
    "                right_on=\"row_id\",\n",
    "            )\n",
    "            semantic_df.rename(columns={\"rule_violation\": \"label\"}, inplace=True)\n",
    "            semantic_df[\"label\"].fillna(0.0, inplace=True)\n",
    "            semantic_df[\"score\"] = semantic_df[\"score\"].clip(-1, 1)\n",
    "\n",
    "            pos_scores = semantic_df.loc[semantic_df[\"label\"] > 0, \"score\"].to_numpy()\n",
    "            neg_scores = semantic_df.loc[semantic_df[\"label\"] < 0, \"score\"].to_numpy()\n",
    "\n",
    "            if len(pos_scores) == 0 and len(neg_scores) == 0:\n",
    "                return 0.5\n",
    "\n",
    "            def stable_sum(values: np.ndarray) -> float:\n",
    "                if len(values) == 0:\n",
    "                    return 0.0\n",
    "                scaled = values / SEMANTIC_TEMPERATURE\n",
    "                max_scaled = scaled.max()\n",
    "                return float(np.exp(scaled - max_scaled).sum())\n",
    "\n",
    "            pos_weight = stable_sum(pos_scores)\n",
    "            neg_weight = stable_sum(-neg_scores)\n",
    "\n",
    "            if pos_weight == 0.0 and neg_weight == 0.0:\n",
    "                return 0.5\n",
    "            return pos_weight / (pos_weight + neg_weight)\n",
    "\n",
    "        tqdm.pandas(desc=f\"Add label for rule={rule}\")\n",
    "        test_part[\"rule_violation\"] = test_part[\"semantic\"].progress_apply(score_to_probability)\n",
    "        result.append(test_part[[\"row_id\", \"rule_violation\"]].copy())\n",
    "\n",
    "    submission = pd.concat(result, axis=0)\n",
    "    return submission\n",
    "\n",
    "\n",
    "def generate_submission():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    test_dataframe = prepare_dataframe(test_dataframe)\n",
    "\n",
    "    submission = get_scores(test_dataframe)\n",
    "    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n",
    "    submission.to_csv(\"submission_qwen3.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_submission()\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:42.284925Z",
     "iopub.execute_input": "2025-10-03T11:55:42.28518Z",
     "iopub.status.idle": "2025-10-03T11:55:42.298878Z",
     "shell.execute_reply.started": "2025-10-03T11:55:42.285163Z",
     "shell.execute_reply": "2025-10-03T11:55:42.298253Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os",
    "import subprocess",
    "import sys",
    "",
    "print(\"=\"*70)",
    "print(\"STEP 3: Running Qwen3 Embedding Semantic Search\")",
    "print(\"=\"*70)",
    "",
    "result = subprocess.run(['python', 'semantic.py'], capture_output=True, text=True)",
    "print(result.stdout)",
    "if result.stderr:",
    "    print(\"STDERR:\", result.stderr, file=sys.stderr)",
    "",
    "if result.returncode != 0:",
    "    print(f\"‚ùå ERROR: semantic.py failed with return code {result.returncode}\")",
    "    sys.exit(1)",
    "",
    "if not os.path.exists('submission_qwen3.csv'):",
    "    print(\"‚ùå ERROR: submission_qwen3.csv was not created!\")",
    "    sys.exit(1)",
    "",
    "print(\"‚úÖ SUCCESS: submission_qwen3.csv created\")",
    "print(\"=\"*70)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:42.29943Z",
     "iopub.execute_input": "2025-10-03T11:55:42.299671Z",
     "iopub.status.idle": "2025-10-03T12:00:56.212473Z",
     "shell.execute_reply.started": "2025-10-03T11:55:42.299656Z",
     "shell.execute_reply": "2025-10-03T12:00:56.211689Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 4. ENSEMBLE RESULT",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd",
    "import numpy as np",
    "import os",
    "import sys",
    "",
    "# Validate that all required submission files exist",
    "required_files = ['submission_qwen.csv', 'submission_qwen3.csv', 'submission_qwen14b.csv']",
    "missing_files = [f for f in required_files if not os.path.exists(f)]",
    "",
    "if missing_files:",
    "    print(\"=\"*70)",
    "    print(\"‚ö†Ô∏è  PREREQUISITE CELLS NOT RUN YET\")",
    "    print(\"=\"*70)",
    "    print(f\"Missing files: {missing_files}\")",
    "    print()",
    "    print(\"This is normal! Please run the previous cells in order:\")",
    "    print()",
    "    print(\"Step 1: Run cells 1-13 (Train & Inference)\")",
    "    print(\"  ‚Üí Generates submission_qwen.csv\")",
    "    print()",
    "    print(\"Step 2: Run cells 14-18 (14B Model)\")",
    "    print(\"  ‚Üí Generates submission_qwen14b.csv\")",
    "    print()",
    "    print(\"Step 3: Run cells 19-24 (Embedding Model)\")",
    "    print(\"  ‚Üí Generates submission_qwen3.csv\")",
    "    print()",
    "    print(\"Step 4: Run THIS cell (Ensemble)\")",
    "    print(\"  ‚Üí Creates final submission.csv\")",
    "    print()",
    "    print(\"TIP: You can also use 'Run All' to execute all cells in order!\")",
    "    print(\"=\"*70)",
    "    raise SystemExit(\"Please run prerequisite cells first (see instructions above)\")",
    "",
    "# Load predictions",
    "q = pd.read_csv('submission_qwen.csv')",
    "l = pd.read_csv('submission_qwen3.csv')",
    "m = pd.read_csv('submission_qwen14b.csv')",
    "",
    "print(f\"‚úì Loaded predictions: {len(q)} rows from each model\")",
    "",
    "# ENHANCEMENT 5: Temperature Scaling for Calibration",
    "def apply_temperature_scaling(predictions, temperature=1.2):",
    "    \"\"\"Apply temperature scaling to calibrate probabilities\"\"\"",
    "    # Convert to logits if needed (assume 0-1 range)",
    "    eps = 1e-7",
    "    predictions_clipped = np.clip(predictions, eps, 1 - eps)",
    "    logits = np.log(predictions_clipped / (1 - predictions_clipped))",
    "    scaled_logits = logits / temperature",
    "    # Convert back to probabilities",
    "    return 1 / (1 + np.exp(-scaled_logits))",
    "",
    "# Apply calibration",
    "print(\"‚úì Applying temperature scaling (T=1.2)...\")",
    "q_cal = apply_temperature_scaling(q['rule_violation'].values, temperature=1.2)",
    "l_cal = apply_temperature_scaling(l['rule_violation'].values, temperature=1.2)",
    "m_cal = apply_temperature_scaling(m['rule_violation'].values, temperature=1.2)",
    "",
    "# Rank normalization on calibrated predictions",
    "rq = pd.Series(q_cal).rank(method='average') / (len(q) + 1)",
    "rl = pd.Series(l_cal).rank(method='average') / (len(l) + 1)",
    "rm = pd.Series(m_cal).rank(method='average') / (len(m) + 1)",
    "",
    "# ENHANCEMENT 3: Optimized Ensemble Weights",
    "# Changed from 50/30/20 to 45/25/30 (more weight to powerful 14B model)",
    "print(\"‚úì Creating optimized ensemble (weights: 45/25/30)...\")",
    "blend = 0.45*rq + 0.25*rl + 0.30*rm",
    "",
    "q['rule_violation'] = blend",
    "q.to_csv('/kaggle/working/submission.csv', index=False)",
    "",
    "print()",
    "print(\"=\"*70)",
    "print(\"‚úÖ ENSEMBLE COMPLETE!\")",
    "print(\"=\"*70)",
    "print(\"Temperature scaling: T=1.2\")",
    "print(\"Ensemble weights: 0.5B=45%, Embeddings=25%, 14B=30%\")",
    "print(f\"Output: /kaggle/working/submission.csv ({len(q)} rows)\")",
    "print(\"=\"*70)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T12:00:56.213636Z",
     "iopub.execute_input": "2025-10-03T12:00:56.214585Z",
     "iopub.status.idle": "2025-10-03T12:00:56.238502Z",
     "shell.execute_reply.started": "2025-10-03T12:00:56.214544Z",
     "shell.execute_reply": "2025-10-03T12:00:56.237971Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\npd.read_csv('/kaggle/working/submission.csv')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T12:00:56.23928Z",
     "iopub.execute_input": "2025-10-03T12:00:56.23959Z",
     "iopub.status.idle": "2025-10-03T12:00:56.262733Z",
     "shell.execute_reply.started": "2025-10-03T12:00:56.239563Z",
     "shell.execute_reply": "2025-10-03T12:00:56.262076Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom utils import get_dataframe_to_train\nfrom constants import DATA_PATH\n\n# --- Â≠¶Áøí„Éá„Éº„Çø„ÅÆÂèñÂæó ---\ntrain_df = get_dataframe_to_train(DATA_PATH)\n\n# --- „É©„Éô„É´„Å®ÁâπÂæ¥„ÅÆÊ∫ñÂÇô ---\ny_true = train_df[\"rule_violation\"]\nX_dummy = np.zeros(len(train_df))  # ÁâπÂæ¥Èáè„Çí‰Ωø„Çè„Å™„ÅÑ„ÉÄ„Éü„ÉºÂàÜÂâ≤\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nauc_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_dummy, y_true), 1):\n    y_train, y_val = y_true.iloc[train_idx], y_true.iloc[val_idx]\n    auc = roc_auc_score(y_val, np.random.rand(len(y_val)))  # ‰ªÆ„Å´„É©„É≥„ÉÄ„É†‰∫àÊ∏¨\n    auc_scores.append(auc)\n    print(f\"Fold {fold}: AUC = {auc:.4f}\")\n\nprint(f\"\\n‚úÖ Mean CV AUC: {np.mean(auc_scores):.4f} ¬± {np.std(auc_scores):.4f}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}