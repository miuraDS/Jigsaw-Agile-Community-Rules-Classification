{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa + Qwen Mega-Ensemble - Strategy A\n",
    "\n",
    "## Overview\n",
    "This notebook combines the best elements from the two highest-performing approaches:\n",
    "- **Experiment 7** (DeBERTa ensemble - 0.917 AUC)\n",
    "- **Experiment 1** (Qwen ensemble - 0.916 AUC)\n",
    "\n",
    "## Strategy: Mega-Ensemble\n",
    "Instead of picking one approach, we create a **mega-ensemble** of 8 models:\n",
    "\n",
    "**From Experiment 7 (DeBERTa approach):**\n",
    "1. DeBERTa v3 base\n",
    "2. DistilRoBERTa\n",
    "3. DeBERTa AUC variant\n",
    "\n",
    "**From Experiment 1 (Qwen approach):**\n",
    "4. Qwen 0.5B with LoRA\n",
    "5. Qwen 14B with LoRA\n",
    "\n",
    "**Shared (both use this):**\n",
    "6. Qwen3 Embeddings with semantic search\n",
    "\n",
    "**Total: 6 distinct model predictions**\n",
    "\n",
    "## Ensemble Strategy\n",
    "We'll use a **two-stage ensemble**:\n",
    "\n",
    "### Stage 1: Within-Method Ensembles\n",
    "- **DeBERTa ensemble**: Blend DeBERTa v3, DistilRoBERTa, DeBERTa AUC (weights from Exp 7)\n",
    "- **Qwen ensemble**: Blend Qwen 0.5B, Qwen 14B, Qwen3 Embeddings (weights from Exp 1)\n",
    "\n",
    "### Stage 2: Meta-Ensemble\n",
    "- Blend the two ensemble outputs with optimized weights\n",
    "- Expected best weights: 55% DeBERTa (slightly better) + 45% Qwen\n",
    "\n",
    "## Expected Performance\n",
    "- **Target**: 0.918-0.920 AUC\n",
    "- **Rationale**: Combining two strong, diverse approaches should capture complementary patterns\n",
    "- **Risk**: Low (both base approaches are proven)\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "This work combines insights from:\n",
    "\n",
    "**Experiment 7: DeBERTa Large 2epochs 1hr (0.917 AUC)**\n",
    "- Author: [itahiro](https://www.kaggle.com/itahiro)\n",
    "- Notebook: https://www.kaggle.com/code/itahiro/deberta-large-2epochs-1hr\n",
    "- Contribution: DeBERTa ensemble architecture, URL semantic extraction\n",
    "\n",
    "**Experiment 1: Qwen Multi-Model Ensemble (0.916 AUC)**\n",
    "- Internal experiments with Qwen 0.5B, 14B, and embeddings\n",
    "- Contribution: LoRA fine-tuning approach, semantic search methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Note\n",
    "\n",
    "Since both Experiment 7 and Experiment 1 notebooks already contain complete implementations for their respective approaches, this notebook focuses on the **meta-ensemble** stage.\n",
    "\n",
    "### Required Pre-requisites:\n",
    "1. Run Experiment 7 notebook to generate:\n",
    "   - `submission_deberta.csv`\n",
    "   - `submission_distilroberta.csv`\n",
    "   - `submission_debertaauc.csv`\n",
    "   - `submission_qwen3.csv` (from semantic search)\n",
    "   - `submission_qwen14b.csv`\n",
    "\n",
    "2. Run Experiment 1 notebook components to generate:\n",
    "   - `submission_qwen05b.csv` (Qwen 0.5B with LoRA)\n",
    "\n",
    "### This Notebook:\n",
    "Combines all 6 predictions using a two-stage ensemble approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load all model predictions\n",
    "print(\"Loading model predictions...\")\n",
    "\n",
    "# DeBERTa-based models (from Experiment 7)\n",
    "deberta = pd.read_csv('submission_deberta.csv')\n",
    "distilroberta = pd.read_csv('submission_distilroberta.csv')\n",
    "deberta_auc = pd.read_csv('submission_debertaauc.csv')\n",
    "\n",
    "# Qwen-based models (from Experiment 1)\n",
    "qwen_05b = pd.read_csv('submission_qwen.csv')  # Qwen 0.5B from Exp 1\n",
    "qwen_14b = pd.read_csv('submission_qwen14b.csv')\n",
    "\n",
    "# Shared semantic search (used by both)\n",
    "qwen3_embeddings = pd.read_csv('submission_qwen3.csv')\n",
    "\n",
    "print(\"All predictions loaded successfully!\")\n",
    "print(f\"Number of test samples: {len(deberta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Create within-method ensembles\n",
    "print(\"\\n=== Stage 1: Within-Method Ensembles ===\")\n",
    "\n",
    "# Rank normalization function\n",
    "def rank_normalize(series):\n",
    "    return series.rank(method='average') / (len(series) + 1)\n",
    "\n",
    "# DeBERTa Ensemble (using Experiment 7 weights: 0.5, 0.1, 0.2 for main components)\n",
    "# Adjusted to exclude Qwen models: renormalize to sum to 1.0\n",
    "# Original: 0.5 DeBERTa + 0.1 DistilRoBERTa + 0.2 DeBERTa AUC (+ 0.1 Qwen3 + 0.1 Qwen14B)\n",
    "# New (DeBERTa only): 0.625 DeBERTa + 0.125 DistilRoBERTa + 0.25 DeBERTa AUC\n",
    "r_deberta = rank_normalize(deberta['rule_violation'])\n",
    "r_distilroberta = rank_normalize(distilroberta['rule_violation'])\n",
    "r_deberta_auc = rank_normalize(deberta_auc['rule_violation'])\n",
    "\n",
    "deberta_ensemble = 0.625 * r_deberta + 0.125 * r_distilroberta + 0.25 * r_deberta_auc\n",
    "\n",
    "print(f\"DeBERTa Ensemble created (3 models)\")\n",
    "print(f\"  Weights: [0.625, 0.125, 0.25]\")\n",
    "print(f\"  Mean: {deberta_ensemble.mean():.4f}, Std: {deberta_ensemble.std():.4f}\")\n",
    "\n",
    "# Qwen Ensemble (using Experiment 1 approach)\n",
    "# Typical weights: 0.5 for 0.5B, 0.3 for embeddings, 0.2 for 14B\n",
    "r_qwen_05b = rank_normalize(qwen_05b['rule_violation'])\n",
    "r_qwen_14b = rank_normalize(qwen_14b['rule_violation'])\n",
    "r_qwen3 = rank_normalize(qwen3_embeddings['rule_violation'])\n",
    "\n",
    "qwen_ensemble = 0.5 * r_qwen_05b + 0.3 * r_qwen3 + 0.2 * r_qwen_14b\n",
    "\n",
    "print(f\"\\nQwen Ensemble created (3 models)\")\n",
    "print(f\"  Weights: [0.5, 0.3, 0.2]\")\n",
    "print(f\"  Mean: {qwen_ensemble.mean():.4f}, Std: {qwen_ensemble.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Meta-Ensemble\n",
    "print(\"\\n=== Stage 2: Meta-Ensemble ===\")\n",
    "\n",
    "# Experiment 7 (DeBERTa) scored 0.917, Experiment 1 (Qwen) scored 0.916\n",
    "# Give slightly more weight to DeBERTa: 55% vs 45%\n",
    "deberta_weight = 0.55\n",
    "qwen_weight = 0.45\n",
    "\n",
    "# Rank normalize the ensemble predictions\n",
    "r_deberta_ens = rank_normalize(deberta_ensemble)\n",
    "r_qwen_ens = rank_normalize(qwen_ensemble)\n",
    "\n",
    "# Final mega-ensemble\n",
    "mega_ensemble = deberta_weight * r_deberta_ens + qwen_weight * r_qwen_ens\n",
    "\n",
    "print(f\"Meta-Ensemble Weights:\")\n",
    "print(f\"  DeBERTa approach: {deberta_weight:.2f} (Exp 7 - 0.917 AUC)\")\n",
    "print(f\"  Qwen approach: {qwen_weight:.2f} (Exp 1 - 0.916 AUC)\")\n",
    "print(f\"\\nFinal Prediction Statistics:\")\n",
    "print(f\"  Mean: {mega_ensemble.mean():.4f}\")\n",
    "print(f\"  Std: {mega_ensemble.std():.4f}\")\n",
    "print(f\"  Min: {mega_ensemble.min():.4f}\")\n",
    "print(f\"  Max: {mega_ensemble.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final submission\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': deberta['row_id'],\n",
    "    'rule_violation': mega_ensemble\n",
    "})\n",
    "\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "\n",
    "print(\"\\n=== Submission Created ===\")\n",
    "print(f\"Saved to: /kaggle/working/submission.csv\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\n=== Expected Performance ===\")\n",
    "print(f\"Target: 0.918-0.920 AUC\")\n",
    "print(f\"Rationale: Combining two proven approaches (0.917 + 0.916) with complementary strengths\")\n",
    "print(f\"Risk Level: Low (both base methods are validated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Ensemble Weights to Try\n",
    "\n",
    "If the 55/45 split doesn't work well, here are alternatives to test:\n",
    "\n",
    "1. **Equal weight (50/50)**: Treats both methods equally\n",
    "2. **Conservative (60/40)**: More weight to DeBERTa since it scored slightly better\n",
    "3. **Aggressive (70/30)**: Heavily favor the better method\n",
    "\n",
    "You can modify the `deberta_weight` and `qwen_weight` variables in the meta-ensemble cell above to test these alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This mega-ensemble notebook:\n",
    "- Combines 6 distinct model predictions\n",
    "- Uses a two-stage ensemble approach\n",
    "- Leverages diversity from both DeBERTa and Qwen methods\n",
    "- Expected to achieve 0.918-0.920 AUC\n",
    "\n",
    "The key insight: Instead of choosing between two good approaches, combine them to capture different patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
