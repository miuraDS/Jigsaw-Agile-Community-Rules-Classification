{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 94635,
     "databundleVersionId": 13121456,
     "sourceType": "competition"
    },
    {
     "sourceId": 12726948,
     "sourceType": "datasetVersion",
     "datasetId": 8044304
    },
    {
     "sourceId": 12762469,
     "sourceType": "datasetVersion",
     "datasetId": 8067935
    },
    {
     "sourceId": 13016855,
     "sourceType": "datasetVersion",
     "datasetId": 8241242
    },
    {
     "sourceId": 13038596,
     "sourceType": "datasetVersion",
     "datasetId": 8256201
    },
    {
     "sourceId": 13044102,
     "sourceType": "datasetVersion",
     "datasetId": 8259759
    },
    {
     "sourceId": 252850661,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 252853424,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 265160037,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 123002,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 103527,
     "modelId": 127747
    },
    {
     "sourceId": 171496,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 145960,
     "modelId": 164048
    },
    {
     "sourceId": 171638,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 146086,
     "modelId": 164048
    },
    {
     "sourceId": 426330,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 347541,
     "modelId": 368803
    },
    {
     "sourceId": 426333,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 347543,
     "modelId": 368803
    },
    {
     "sourceId": 426337,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 347547,
     "modelId": 368803
    },
    {
     "sourceId": 523492,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 411182,
     "modelId": 429004
    }
   ],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "\n### References\n\n*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch\n*   [https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference](https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference)\n*   https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### I want to say thanks to @neibyr for your interesting idea: [Retrieve by Qwen3Embedding](http://https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b) ",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "This version changes the lr for training Qwen 3 0.5b. ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:40:47.599292Z",
     "iopub.execute_input": "2025-10-03T11:40:47.599942Z",
     "iopub.status.idle": "2025-10-03T11:40:48.786208Z",
     "shell.execute_reply.started": "2025-10-03T11:40:47.599917Z",
     "shell.execute_reply": "2025-10-03T11:40:48.785398Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 1. Test time train Qwen 2.5 0.5b",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%writefile constants.py\nBASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\nLORA_PATH = \"output/\"\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\nPOSITIVE_ANSWER = \"Yes\"\nNEGATIVE_ANSWER = \"No\"\nCOMPLETE_PHRASE = \"Answer:\"\nBASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:40:48.787807Z",
     "iopub.execute_input": "2025-10-03T11:40:48.788069Z",
     "iopub.status.idle": "2025-10-03T11:40:48.79448Z",
     "shell.execute_reply.started": "2025-10-03T11:40:48.788045Z",
     "shell.execute_reply": "2025-10-03T11:40:48.793539Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile rule_knowledge.py\n\nRULE_CANON_SHORT = {\n    \"public rule 0\": \"[ADVERTISING]\",\n    \"public rule 1\": \"[LEGAL ADVICE]\",\n    \"private rule 0\": \"[FINANCIAL ADVICE]\",\n    \"private rule 1\": \"[MEDICAL ADVICE]\",\n    \"private rule 2\": \"[ILLEGAL ACTIVITY]\",\n    \"private rule 3\": \"[SPOILERS]\",\n}\n\nKEYWORD_FALLBACK = [\n    (\"advertis|referral|promo|spam\", \"public rule 0\"),\n    (\"legal\", \"public rule 1\"),\n    (\"financ|invest|tax|career\", \"private rule 0\"),\n    (\"medical|diagnos|treat\", \"private rule 1\"),\n    (\"illegal|drug|violence|exploit|theft|crime\", \"private rule 2\"),\n    (\"spoiler\", \"private rule 3\"),\n]\n\nimport re\n\ndef canonicalize_rule(rule_str: str):\n    s = (rule_str or \"\").strip().lower()\n    # \u76f4\u63a5\u4e00\u81f4\n    for k in RULE_CANON_SHORT:\n        if k in s:\n            return RULE_CANON_SHORT[k]\n    # \u30ad\u30fc\u30ef\u30fc\u30c9\u3067\u30d5\u30a9\u30fc\u30eb\u30d0\u30c3\u30af\n    for pat, key in KEYWORD_FALLBACK:\n        if re.search(pat, s):\n            return RULE_CANON_SHORT[key]\n    return None",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:40:48.795548Z",
     "iopub.execute_input": "2025-10-03T11:40:48.795849Z",
     "iopub.status.idle": "2025-10-03T11:40:48.813833Z",
     "shell.execute_reply.started": "2025-10-03T11:40:48.795825Z",
     "shell.execute_reply": "2025-10-03T11:40:48.813122Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile text_cleaning.py\nimport regex as re  # pip install regex \u304c\u5fc5\u8981\n\n# Unicode Emoji \u5168\u5bfe\u5fdc\uff08ZWJ, \u30b9\u30ad\u30f3\u30c8\u30fc\u30f3\u3082\u542b\u3080\uff09\n_EMOJI_RE = re.compile(r\"\\p{Emoji}+\")\n\n# Markdown\u88c5\u98fe\u30d1\u30bf\u30fc\u30f3\n_MD_PATTERNS = [\n    r\"\\*\\*(.*?)\\*\\*\",   # **bold**\n    r\"\\*(.*?)\\*\",       # *italic/bold*\n    r\"__(.*?)__\",       # __italic__\n    r\"_(.*?)_\",         # _italic_\n    r\"`(.*?)`\",         # `inline code`\n    r\"#+\\s+\",           # # Heading\n    r\">+\\s+\",           # > quote\n    r\"-{3,}\",           # --- hr\n]\n\n_MD_RE = re.compile(\"|\".join(_MD_PATTERNS), flags=re.MULTILINE)\n\ndef strip_emojis_kaomoji(text: str) -> str:\n    if not text:\n        return text\n    s = str(text)\n\n    # \u7d75\u6587\u5b57\u524a\u9664\n    s = _EMOJI_RE.sub(\"\", s)\n\n    # Markdown\u88c5\u98fe\u524a\u9664\uff08\u5185\u5bb9\u306f\u6b8b\u3059\uff09\n    s = _MD_RE.sub(lambda m: m.group(1) if m.lastindex else \"\", s)\n\n    # \u7a7a\u767d\u30fb\u6539\u884c\u306e\u6574\u5f62\n    s = re.sub(r\"[ \\t\\r\\f\\v]+\", \" \", s)\n    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n    return s\n\n\nif __name__ == \"__main__\":\n    sample = \"\"\"\n# **Huge SALE!!!**\nGet *FREE* stuff \ud83d\udc49\ud83d\udc49 https://spam.com\n> Only today!!!\n\ud83d\ude0a\ud83d\udd25\ud83d\ude80\n\"\"\"\n    print(\"Before:\", sample)\n    print(\"After :\", strip_emojis_kaomoji(sample))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:01.072926Z",
     "iopub.execute_input": "2025-10-03T11:41:01.073205Z",
     "iopub.status.idle": "2025-10-03T11:41:01.07887Z",
     "shell.execute_reply.started": "2025-10-03T11:41:01.073182Z",
     "shell.execute_reply": "2025-10-03T11:41:01.078082Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "from rule_knowledge import canonicalize_rule\n",
    "from text_cleaning import strip_emojis_kaomoji as _strip\n",
    "import random, numpy as np\n",
    "random.seed(42); np.random.seed(42)\n",
    "\n",
    "def _sz(x):  # sanitize helper\n",
    "    return _strip(\"\" if pd.isna(x) else str(x))\n",
    "\n",
    "def build_prompt(row):\n",
    "    rule_raw = _sz(row[\"rule\"])\n",
    "    body = _sz(row[\"body\"])\n",
    "    subreddit = _sz(row[\"subreddit\"])\n",
    "    pos_ex = _sz(row[\"positive_example\"])\n",
    "    neg_ex = _sz(row[\"negative_example\"])\n",
    "\n",
    "    canon = canonicalize_rule(rule_raw)\n",
    "    rule_block = f\"Rule: {rule_raw}\\n\"\n",
    "    if canon:\n",
    "        rule_block += f\"Canonical Definition: {canon}\\n\"\n",
    "\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{subreddit}\n",
    "{rule_block}\n",
    "Examples:\n",
    "1) {pos_ex}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {neg_ex}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {body}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    import numpy as np\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "\n",
    "    train_df = train_dataset[[\n",
    "        \"body\",\"rule\",\"subreddit\",\"rule_violation\",\n",
    "        \"positive_example_1\",\"positive_example_2\",\n",
    "        \"negative_example_1\",\"negative_example_2\"\n",
    "    ]].copy()\n",
    "\n",
    "    train_df[\"positive_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"positive_example_1\"], train_df[\"positive_example_2\"]\n",
    "    )\n",
    "    train_df[\"negative_example\"] = np.where(\n",
    "        np.random.rand(len(train_df)) < 0.5,\n",
    "        train_df[\"negative_example_1\"], train_df[\"negative_example_2\"]\n",
    "    )\n",
    "    train_df.drop(columns=[\n",
    "        \"positive_example_1\",\"positive_example_2\",\n",
    "        \"negative_example_1\",\"negative_example_2\"\n",
    "    ], inplace=True)\n",
    "\n",
    "    # \u3053\u3053\u3067\u4e3b\u8981\u30c6\u30ad\u30b9\u30c8\u5217\u3092\u30af\u30ea\u30fc\u30cb\u30f3\u30b0\uff08\u5b66\u7fd2\u30c7\u30fc\u30bf\uff09\n",
    "    for c in [\"body\",\"rule\",\"subreddit\",\"positive_example\",\"negative_example\"]:\n",
    "        train_df[c] = train_df[c].astype(str).map(_sz)\n",
    "\n",
    "    flatten.append(train_df)\n",
    "\n",
    "    # \u30c6\u30b9\u30c8\u4f8b\u793a\u304b\u3089\u306e flatten \u62e1\u5f35\uff08\u30e9\u30d9\u30eb\u4ed8\u4e0e\uff09\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 2+1):\n",
    "            sub = test_dataset[[\n",
    "                \"rule\",\"subreddit\",\n",
    "                \"positive_example_1\",\"positive_example_2\",\n",
    "                \"negative_example_1\",\"negative_example_2\"\n",
    "            ]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"\n",
    "                sub[\"body\"] = sub[body_col]\n",
    "                sub[\"positive_example\"] = sub[other_positive_col]\n",
    "                sub[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub)) < 0.5, sub[\"negative_example_1\"], sub[\"negative_example_2\"]\n",
    "                )\n",
    "                sub[\"rule_violation\"] = 1\n",
    "            else:\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub[\"body\"] = sub[body_col]\n",
    "                sub[\"negative_example\"] = sub[other_negative_col]\n",
    "                sub[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub)) < 0.5, sub[\"positive_example_1\"], sub[\"positive_example_2\"]\n",
    "                )\n",
    "                sub[\"rule_violation\"] = 0\n",
    "\n",
    "            sub.drop(columns=[\n",
    "                \"positive_example_1\",\"positive_example_2\",\"negative_example_1\",\"negative_example_2\"\n",
    "            ], inplace=True)\n",
    "\n",
    "            # \u4e3b\u8981\u5217\u3092\u30af\u30ea\u30fc\u30cb\u30f3\u30b0\uff08\u62e1\u5f35\u30c7\u30fc\u30bf\uff09\n",
    "            for c in [\"body\",\"rule\",\"subreddit\",\"positive_example\",\"negative_example\"]:\n",
    "                sub[c] = sub[c].astype(str).map(_sz)\n",
    "\n",
    "            flatten.append(sub)\n",
    "\n",
    "    df = pd.concat(flatten, axis=0).drop_duplicates(ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map({1: POSITIVE_ANSWER, 0: NEGATIVE_ANSWER})\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    dataset = Dataset.from_pandas(dataframe[columns])\n",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n",
    "    return dataset"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:03.147572Z",
     "iopub.execute_input": "2025-10-03T11:41:03.147909Z",
     "iopub.status.idle": "2025-10-03T11:41:03.155991Z",
     "shell.execute_reply.started": "2025-10-03T11:41:03.147886Z",
     "shell.execute_reply": "2025-10-03T11:41:03.15493Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile train.py\nimport pandas as pd\n\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom tqdm.auto import tqdm\nfrom transformers.utils import is_torch_bf16_gpu_available\nfrom utils import build_dataset, get_dataframe_to_train\nfrom constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n\n\ndef main():\n    dataframe = get_dataframe_to_train(DATA_PATH)\n    train_dataset = build_dataset(dataframe)\n    \n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.1,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\",\n    )\n    \n    training_args = SFTConfig(\n        num_train_epochs=1,\n        \n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        \n        optim=\"paged_adamw_8bit\",\n        learning_rate=1e-4, #keep high, lora usually likes high. \n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        \n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.03,\n        \n        bf16=is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        \n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n        save_strategy=\"no\",\n        report_to=\"none\",\n    \n        completion_only_loss=True,\n        packing=False,\n        remove_unused_columns=False,\n    )\n    \n    trainer = SFTTrainer(\n        BASE_MODEL_PATH,\n        args=training_args,\n        train_dataset=train_dataset,\n        peft_config=lora_config,\n    )\n    \n    trainer.train()\n    trainer.save_model(LORA_PATH)\n\n\nif __name__ == \"__main__\":\n    main()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:03.956569Z",
     "iopub.execute_input": "2025-10-03T11:41:03.956894Z",
     "iopub.status.idle": "2025-10-03T11:41:03.962312Z",
     "shell.execute_reply.started": "2025-10-03T11:41:03.956873Z",
     "shell.execute_reply": "2025-10-03T11:41:03.961493Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile inference.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import multiprocessing as mp\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import vllm\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "from utils import build_dataset\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ExampleVariant:\n",
    "    pos_col: str\n",
    "    neg_col: str\n",
    "\n",
    "\n",
    "EXAMPLE_VARIANTS: Tuple[ExampleVariant, ...] = (\n",
    "    ExampleVariant(\"positive_example_1\", \"negative_example_1\"),\n",
    "    ExampleVariant(\"positive_example_1\", \"negative_example_2\"),\n",
    "    ExampleVariant(\"positive_example_2\", \"negative_example_1\"),\n",
    "    ExampleVariant(\"positive_example_2\", \"negative_example_2\"),\n",
    ")\n",
    "\n",
    "\n",
    "def _build_llm() -> vllm.LLM:\n",
    "    return vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization=\"gptq\",\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.98,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "    )\n",
    "\n",
    "\n",
    "def _run_inference_on_device(df_slice: pd.DataFrame) -> pd.DataFrame:\n",
    "    llm = _build_llm()\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "\n",
    "    dataset = build_dataset(df_slice)\n",
    "    prompts = dataset[\"prompt\"]\n",
    "    meta = df_slice[[\"row_id\", \"tta_variant\"]].reset_index(drop=True)\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        prompts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH),\n",
    "    )\n",
    "\n",
    "    records = []\n",
    "    for out in outputs:\n",
    "        logprob_map = {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        records.append(logprob_map)\n",
    "\n",
    "    predictions = pd.DataFrame(records)\n",
    "    if POSITIVE_ANSWER not in predictions:\n",
    "        predictions[POSITIVE_ANSWER] = -1e9\n",
    "    if NEGATIVE_ANSWER not in predictions:\n",
    "        predictions[NEGATIVE_ANSWER] = -1e9\n",
    "    predictions = predictions[[POSITIVE_ANSWER, NEGATIVE_ANSWER]]\n",
    "    predictions[[\"row_id\", \"tta_variant\"]] = meta\n",
    "    return predictions[[\"row_id\", \"tta_variant\", POSITIVE_ANSWER, NEGATIVE_ANSWER]]\n",
    "\n",
    "\n",
    "def _worker(device_id: int, df_slice: pd.DataFrame, return_dict):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n",
    "    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n",
    "    return_dict[device_id] = _run_inference_on_device(df_slice)\n",
    "\n",
    "\n",
    "def _distributed_inference(expanded_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num_devices = max(torch.cuda.device_count(), 1)\n",
    "    device_ids = list(range(num_devices))\n",
    "    index_splits = np.array_split(np.arange(len(expanded_df)), num_devices)\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "    processes: List[mp.Process] = []\n",
    "\n",
    "    for device_id, indices in zip(device_ids, index_splits):\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        df_slice = expanded_df.iloc[indices].reset_index(drop=True)\n",
    "        p = mp.Process(target=_worker, args=(device_id, df_slice, return_dict))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    combined = pd.concat([return_dict[idx] for idx in sorted(return_dict.keys())], ignore_index=True)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def _prepare_variants(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    variants = []\n",
    "    for variant_id, config in enumerate(EXAMPLE_VARIANTS):\n",
    "        augmented = df.copy()\n",
    "        augmented[\"positive_example\"] = augmented[config.pos_col]\n",
    "        augmented[\"negative_example\"] = augmented[config.neg_col]\n",
    "        augmented[\"tta_variant\"] = variant_id\n",
    "        variants.append(augmented)\n",
    "    return pd.concat(variants, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "def _logits_to_probabilities(predictions: pd.DataFrame) -> pd.Series:\n",
    "    log_pos = predictions[POSITIVE_ANSWER].to_numpy()\n",
    "    log_neg = predictions[NEGATIVE_ANSWER].to_numpy()\n",
    "    max_log = np.maximum(log_pos, log_neg)\n",
    "    pos_exp = np.exp(log_pos - max_log)\n",
    "    neg_exp = np.exp(log_neg - max_log)\n",
    "    return pos_exp / np.clip(pos_exp + neg_exp, a_min=1e-12, a_max=None)\n",
    "\n",
    "\n",
    "def main():\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    expanded_df = _prepare_variants(test_df)\n",
    "\n",
    "    predictions = _distributed_inference(expanded_df)\n",
    "    predictions[\"prob_yes\"] = _logits_to_probabilities(predictions.fillna(-1e9))\n",
    "\n",
    "    agg = (\n",
    "        predictions\n",
    "        .groupby(\"row_id\", as_index=False)[\"prob_yes\"]\n",
    "        .mean()\n",
    "        .rename(columns={\"prob_yes\": \"rule_violation\"})\n",
    "    )\n",
    "\n",
    "    agg[\"rule_violation\"] = agg[\"rule_violation\"].rank(method=\"average\") / (len(agg) + 1)\n",
    "    agg.to_csv(\"submission_qwen.csv\", index=False)\n",
    "    print(\"\u2705 Saved submission_qwen.csv with TTA averaging\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:06.296527Z",
     "iopub.execute_input": "2025-10-03T11:41:06.297322Z",
     "iopub.status.idle": "2025-10-03T11:41:06.303711Z",
     "shell.execute_reply.started": "2025-10-03T11:41:06.297282Z",
     "shell.execute_reply": "2025-10-03T11:41:06.302709Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile accelerate_config.yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 4\n  gradient_clipping: 1.0\n  train_batch_size: 64\n  train_micro_batch_size_per_gpu: 4\n  \n  zero_stage: 2\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: false\n  \n  stage3_gather_16bit_weights_on_model_save: false\n  stage3_max_live_parameters: 1e8\n  stage3_max_reuse_distance: 1e8\n  stage3_prefetch_bucket_size: 5e7\n  stage3_param_persistence_threshold: 1e5\n  \n  zero_allow_untested_optimizer: true\n  zero_force_ds_cpu_optimizer: false\n  \n  fp16:\n    enabled: true\n    loss_scale: 0\n    initial_scale_power: 16\n    loss_scale_window: 1000\n    hysteresis: 2\n    min_loss_scale: 1\n  \ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_config:\n  dynamo_backend: INDUCTOR\n  dynamo_use_fullgraph: false\n  dynamo_use_dynamic: false\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:06.426647Z",
     "iopub.execute_input": "2025-10-03T11:41:06.427224Z",
     "iopub.status.idle": "2025-10-03T11:41:06.432049Z",
     "shell.execute_reply.started": "2025-10-03T11:41:06.427203Z",
     "shell.execute_reply": "2025-10-03T11:41:06.431378Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!accelerate launch --config_file accelerate_config.yaml train.py",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:41:09.647206Z",
     "iopub.execute_input": "2025-10-03T11:41:09.647913Z",
     "iopub.status.idle": "2025-10-03T11:50:51.660906Z",
     "shell.execute_reply.started": "2025-10-03T11:41:09.647885Z",
     "shell.execute_reply": "2025-10-03T11:50:51.65984Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!python inference.py",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:50:51.66286Z",
     "iopub.execute_input": "2025-10-03T11:50:51.663166Z",
     "iopub.status.idle": "2025-10-03T11:52:08.854507Z",
     "shell.execute_reply.started": "2025-10-03T11:50:51.663136Z",
     "shell.execute_reply": "2025-10-03T11:52:08.853764Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!head submission_qwen.csv",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:52:08.855517Z",
     "iopub.execute_input": "2025-10-03T11:52:08.855867Z",
     "iopub.status.idle": "2025-10-03T11:52:08.976243Z",
     "shell.execute_reply.started": "2025-10-03T11:52:08.855838Z",
     "shell.execute_reply": "2025-10-03T11:52:08.975405Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Qwen2.5 14B GPTQ Int4 Inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ! mkdir -p /tmp/src",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:52:08.978704Z",
     "iopub.execute_input": "2025-10-03T11:52:08.979453Z",
     "iopub.status.idle": "2025-10-03T11:52:08.983308Z",
     "shell.execute_reply.started": "2025-10-03T11:52:08.979424Z",
     "shell.execute_reply": "2025-10-03T11:52:08.982328Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%writefile infer_qwen.py\nimport os, math, pandas as pd, torch, vllm, numpy as np\nfrom typing import List, Dict\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom vllm.lora.request import LoRARequest\nfrom rule_knowledge import canonicalize_rule\nfrom text_cleaning import strip_emojis_kaomoji as _strip\n\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\nMODEL_NAME = \"/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1\"\nLORA_PATH  = \"/kaggle/input/lora_14b_gptq_1epoch_r32/keras/default/1\"\n\nGPU_MEM_UTIL = 0.90\nMAX_MODEL_LEN = 1036\nCHUNK_SIZE = 64\nMAX_TOK_BODY = 128\nMAX_TOK_EX   = 64\nMAX_TOK_RULE = 64\nSEED = 42\n\nPOS = \"Yes\"; NEG = \"No\"\n\ndef build_llm():\n    return vllm.LLM(\n        MODEL_NAME,\n        quantization=\"gptq\",\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=GPU_MEM_UTIL,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=MAX_MODEL_LEN,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=32,\n    )\n\ndef truncate_by_tokens(text: str, tokenizer, max_tokens: int) -> str:\n    if not text: return \"\"\n    ids = tokenizer.encode(text, add_special_tokens=False)\n    return text if len(ids) <= max_tokens else tokenizer.decode(ids[:max_tokens], skip_special_tokens=True)\n\nSYS_PROMPT = (\n    \"You are a strict rule-violation classifier for Reddit comments.\\n\"\n    \"Decide if the comment violates the given rule.\\n\"\n    \"- Use ONLY the rule (including the canonical definition) and the provided examples.\\n\"\n    \"- Ignore meme slang unless directly relevant to the rule.\\n\"\n    \"- If uncertain, answer 'No'.\\n\"\n    \"- Output EXACTLY one word: 'Yes' or 'No'.\"\n)\n\ndef build_prompt_rows(df: pd.DataFrame, tokenizer) -> List[str]:\n    prompts = []\n    apply_chat = lambda messages: tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\n    for _, row in df.iterrows():\n        # \u307e\u305a\u30af\u30ea\u30fc\u30f3\n        subreddit = _strip(row.get(\"subreddit\",\"\"))\n        rule_raw  = _strip(row.get(\"rule\",\"\"))\n        pos1 = _strip(row.get(\"positive_example_1\",\"\"))\n        pos2 = _strip(row.get(\"positive_example_2\",\"\"))\n        neg1 = _strip(row.get(\"negative_example_1\",\"\"))\n        neg2 = _strip(row.get(\"negative_example_2\",\"\"))\n        body = _strip(row.get(\"body\",\"\"))\n\n        canon = canonicalize_rule(rule_raw)\n        rule_block = f\"Rule: {rule_raw}\\n\"\n        if canon:\n            rule_block += f\"Canonical Definition: {canon}\\n\"\n\n        # \u30c8\u30fc\u30af\u30f3\u5236\u9650\n        rule_block = truncate_by_tokens(rule_block, tokenizer, MAX_TOK_RULE)\n        pos1 = truncate_by_tokens(pos1, tokenizer, MAX_TOK_EX)\n        pos2 = truncate_by_tokens(pos2, tokenizer, MAX_TOK_EX)\n        neg1 = truncate_by_tokens(neg1, tokenizer, MAX_TOK_EX)\n        neg2 = truncate_by_tokens(neg2, tokenizer, MAX_TOK_EX)\n        body = truncate_by_tokens(body, tokenizer, MAX_TOK_BODY)\n\n        user_text = (\n            f\"Subreddit: r/{subreddit}\\n\"\n            f\"{rule_block}\\n\"\n            f\"Examples of violation (Yes):\\n1) {pos1}\\n2) {pos2}\\n\\n\"\n            f\"Examples of non-violation (No):\\n3) {neg1}\\n4) {neg2}\\n\\n\"\n            f\"Target comment:\\n5) {body}\\n\"\n        )\n\n        prompt = apply_chat([\n            {\"role\":\"system\",\"content\":SYS_PROMPT},\n            {\"role\":\"user\",\"content\":user_text},\n        ]) + \"Answer:\"\n        prompts.append(prompt)\n    return prompts\n\ndef run_chunked_inference(llm, tokenizer, prompts: List[str]) -> List[Dict[str, float]]:\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POS, NEG])\n    all_lp = []\n    for s in range(0, len(prompts), CHUNK_SIZE):\n        batch = prompts[s:s+CHUNK_SIZE]\n        outs = llm.generate(\n            batch,\n            vllm.SamplingParams(skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=2),\n            use_tqdm=True,\n            lora_request=LoRARequest(\"default\", 1, LORA_PATH),\n        )\n        for out in outs:\n            lp_map = {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n            all_lp.append({POS: lp_map.get(POS, -1e9), NEG: lp_map.get(NEG, -1e9)})\n    return all_lp\n\ndef main():\n    torch.manual_seed(SEED); np.random.seed(SEED)\n    df = pd.read_csv(DATA_PATH)\n    llm = build_llm()\n    tokenizer = llm.get_tokenizer()\n\n    prompts = build_prompt_rows(df, tokenizer)\n    logprobs = run_chunked_inference(llm, tokenizer, prompts)\n\n    mat = pd.DataFrame(logprobs)[[POS, NEG]]\n    df = pd.concat([df, mat], axis=1)\n    df[\"logit_diff\"] = df[POS] - df[NEG]\n    df[\"rule_violation\"] = 1.0 / (1.0 + np.exp(-df[\"logit_diff\"]))\n    df[[\"row_id\",\"rule_violation\"]].to_csv(\"submission_qwen14b.csv\", index=False)\n    print(\"\u2705 Saved submission_qwen14b.csv\")\n\nif __name__ == \"__main__\":\n    main()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:52:08.984421Z",
     "iopub.execute_input": "2025-10-03T11:52:08.984716Z",
     "iopub.status.idle": "2025-10-03T11:52:08.998564Z",
     "shell.execute_reply.started": "2025-10-03T11:52:08.984688Z",
     "shell.execute_reply": "2025-10-03T11:52:08.997723Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# %cd /tmp\n!python infer_qwen.py",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:52:08.999515Z",
     "iopub.execute_input": "2025-10-03T11:52:08.999735Z",
     "iopub.status.idle": "2025-10-03T11:55:41.896229Z",
     "shell.execute_reply.started": "2025-10-03T11:52:08.999719Z",
     "shell.execute_reply": "2025-10-03T11:55:41.895171Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Qwen3 0.6b Embedding",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport pandas as pd",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:41.897434Z",
     "iopub.execute_input": "2025-10-03T11:55:41.897714Z",
     "iopub.status.idle": "2025-10-03T11:55:42.262874Z",
     "shell.execute_reply.started": "2025-10-03T11:55:41.89769Z",
     "shell.execute_reply": "2025-10-03T11:55:42.262231Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile constants.py\n",
    "EMBDEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\n",
    "MODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/blob/main/config_sentence_transformers.json\n",
    "EMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\n",
    "Query:\"\n",
    "\n",
    "CLEAN_TEXT = True\n",
    "TOP_K = 2000\n",
    "BATCH_SIZE = 128\n",
    "SEMANTIC_TEMPERATURE = 0.2\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:42.263597Z",
     "iopub.execute_input": "2025-10-03T11:55:42.26402Z",
     "iopub.status.idle": "2025-10-03T11:55:42.269274Z",
     "shell.execute_reply.started": "2025-10-03T11:55:42.264Z",
     "shell.execute_reply": "2025-10-03T11:55:42.26836Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile utils.py\n",
    "import pandas as pd\n",
    "import torch.distributed as dist\n",
    "from datasets import Dataset\n",
    "from cleantext import clean\n",
    "from tqdm.auto import tqdm\n",
    "from text_cleaning import strip_emojis_kaomoji as _strip\n",
    "from constants import CLEAN_TEXT\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n",
    "\n",
    "def cleaner(text):\n",
    "    # clean-text \u3067URL/EMAIL/PHONE\u7b49\u3092\u30de\u30b9\u30af\u3001Unicode\u6574\u5f62\n",
    "    s = clean(\n",
    "        text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=True,           # \u7d75\u6587\u5b57\u306f\u57fa\u672c\u3053\u3053\u3067\u843d\u3061\u308b\n",
    "        lower=False,\n",
    "        no_line_breaks=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=False,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        lang=\"en\",\n",
    "    )\n",
    "    # ASCII\u7cfb\u306e\u9854\u6587\u5b57\u3084\u6b8b\u7559\u3092\u8ffd\u52a0\u3067\u9664\u53bb\n",
    "    s = _strip(s)\n",
    "    return s\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "    flatten.append(train_dataset[[\"body\",\"rule\",\"subreddit\",\"rule_violation\"]])\n",
    "\n",
    "    for violation_type in [\"positive\",\"negative\"]:\n",
    "        for i in range(1,3):\n",
    "            sub = test_dataset[[f\"{violation_type}_example_{i}\",\"rule\",\"subreddit\"]].copy()\n",
    "            sub = sub.rename(columns={f\"{violation_type}_example_{i}\":\"body\"})\n",
    "            sub[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n",
    "            flatten.append(sub)\n",
    "\n",
    "    df = pd.concat(flatten, axis=0).drop_duplicates(ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def prepare_dataframe(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "    if CLEAN_TEXT:\n",
    "        tqdm.pandas(desc=\"cleaner\")\n",
    "        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n",
    "\n",
    "    if \"rule_violation\" in dataframe.columns:\n",
    "        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map({1: 1, 0: -1})\n",
    "    return dataframe"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:42.270311Z",
     "iopub.execute_input": "2025-10-03T11:55:42.270569Z",
     "iopub.status.idle": "2025-10-03T11:55:42.282914Z",
     "shell.execute_reply.started": "2025-10-03T11:55:42.270546Z",
     "shell.execute_reply": "2025-10-03T11:55:42.282229Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile semantic.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search, dot_score\n",
    "from tqdm.auto import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "from utils import get_dataframe_to_train, prepare_dataframe\n",
    "from constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH, SEMANTIC_TEMPERATURE\n",
    "\n",
    "\n",
    "def get_scores(test_dataframe):\n",
    "    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n",
    "\n",
    "    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n",
    "    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n",
    "\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n",
    "    print('Done loading model!')\n",
    "\n",
    "    result = []\n",
    "    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=\"Generate scores for each rule\"):\n",
    "        test_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n",
    "        corpus_part = corpus_part.reset_index(names=\"row_id\")\n",
    "\n",
    "        query_embeddings = embedding_model.encode(\n",
    "            sentences=test_part[\"prompt\"].tolist(),\n",
    "            prompt=EMBEDDING_MODEL_QUERY,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "        document_embeddings = embedding_model.encode(\n",
    "            sentences=corpus_part[\"prompt\"].tolist(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_tensor=True,\n",
    "            device=\"cuda\",\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "        test_part[\"semantic\"] = semantic_search(\n",
    "            query_embeddings,\n",
    "            document_embeddings,\n",
    "            top_k=TOP_K,\n",
    "            score_function=dot_score,\n",
    "        )\n",
    "\n",
    "        def score_to_probability(semantic):\n",
    "            semantic_df = pd.DataFrame(semantic)\n",
    "            semantic_df = semantic_df.merge(\n",
    "                corpus_part[[\"row_id\", \"rule_violation\"]],\n",
    "                how=\"left\",\n",
    "                left_on=\"corpus_id\",\n",
    "                right_on=\"row_id\",\n",
    "            )\n",
    "            semantic_df.rename(columns={\"rule_violation\": \"label\"}, inplace=True)\n",
    "            semantic_df[\"label\"].fillna(0.0, inplace=True)\n",
    "            semantic_df[\"score\"] = semantic_df[\"score\"].clip(-1, 1)\n",
    "\n",
    "            pos_scores = semantic_df.loc[semantic_df[\"label\"] > 0, \"score\"].to_numpy()\n",
    "            neg_scores = semantic_df.loc[semantic_df[\"label\"] < 0, \"score\"].to_numpy()\n",
    "\n",
    "            if len(pos_scores) == 0 and len(neg_scores) == 0:\n",
    "                return 0.5\n",
    "\n",
    "            def stable_sum(values: np.ndarray) -> float:\n",
    "                if len(values) == 0:\n",
    "                    return 0.0\n",
    "                scaled = values / SEMANTIC_TEMPERATURE\n",
    "                max_scaled = scaled.max()\n",
    "                return float(np.exp(scaled - max_scaled).sum())\n",
    "\n",
    "            pos_weight = stable_sum(pos_scores)\n",
    "            neg_weight = stable_sum(-neg_scores)\n",
    "\n",
    "            if pos_weight == 0.0 and neg_weight == 0.0:\n",
    "                return 0.5\n",
    "            return pos_weight / (pos_weight + neg_weight)\n",
    "\n",
    "        tqdm.pandas(desc=f\"Add label for rule={rule}\")\n",
    "        test_part[\"rule_violation\"] = test_part[\"semantic\"].progress_apply(score_to_probability)\n",
    "        result.append(test_part[[\"row_id\", \"rule_violation\"]].copy())\n",
    "\n",
    "    submission = pd.concat(result, axis=0)\n",
    "    return submission\n",
    "\n",
    "\n",
    "def generate_submission():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    test_dataframe = prepare_dataframe(test_dataframe)\n",
    "\n",
    "    submission = get_scores(test_dataframe)\n",
    "    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n",
    "    submission.to_csv(\"submission_qwen3.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_submission()\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:42.284925Z",
     "iopub.execute_input": "2025-10-03T11:55:42.28518Z",
     "iopub.status.idle": "2025-10-03T11:55:42.298878Z",
     "shell.execute_reply.started": "2025-10-03T11:55:42.285163Z",
     "shell.execute_reply": "2025-10-03T11:55:42.298253Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!python semantic.py",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T11:55:42.29943Z",
     "iopub.execute_input": "2025-10-03T11:55:42.299671Z",
     "iopub.status.idle": "2025-10-03T12:00:56.212473Z",
     "shell.execute_reply.started": "2025-10-03T11:55:42.299656Z",
     "shell.execute_reply": "2025-10-03T12:00:56.211689Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 4. ENSEMBLE RESULT",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\n\nq = pd.read_csv('submission_qwen.csv')\nl = pd.read_csv('submission_qwen3.csv')\nm = pd.read_csv('submission_qwen14b.csv')\n\n\nrq = q['rule_violation'].rank(method='average') / (len(q)+1)\nrl = l['rule_violation'].rank(method='average') / (len(l)+1)\nrm = m['rule_violation'].rank(method='average') / (len(m)+1)\n\n\nblend = 0.5*rq + 0.3*rl + 0.2*rm   # or tune the rank-weights with a tiny grid using OOF\nq['rule_violation'] = blend\nq.to_csv('/kaggle/working/submission.csv', index=False)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T12:00:56.213636Z",
     "iopub.execute_input": "2025-10-03T12:00:56.214585Z",
     "iopub.status.idle": "2025-10-03T12:00:56.238502Z",
     "shell.execute_reply.started": "2025-10-03T12:00:56.214544Z",
     "shell.execute_reply": "2025-10-03T12:00:56.237971Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\npd.read_csv('/kaggle/working/submission.csv')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-03T12:00:56.23928Z",
     "iopub.execute_input": "2025-10-03T12:00:56.23959Z",
     "iopub.status.idle": "2025-10-03T12:00:56.262733Z",
     "shell.execute_reply.started": "2025-10-03T12:00:56.239563Z",
     "shell.execute_reply": "2025-10-03T12:00:56.262076Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom utils import get_dataframe_to_train\nfrom constants import DATA_PATH\n\n# --- \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u53d6\u5f97 ---\ntrain_df = get_dataframe_to_train(DATA_PATH)\n\n# --- \u30e9\u30d9\u30eb\u3068\u7279\u5fb4\u306e\u6e96\u5099 ---\ny_true = train_df[\"rule_violation\"]\nX_dummy = np.zeros(len(train_df))  # \u7279\u5fb4\u91cf\u3092\u4f7f\u308f\u306a\u3044\u30c0\u30df\u30fc\u5206\u5272\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nauc_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_dummy, y_true), 1):\n    y_train, y_val = y_true.iloc[train_idx], y_true.iloc[val_idx]\n    auc = roc_auc_score(y_val, np.random.rand(len(y_val)))  # \u4eee\u306b\u30e9\u30f3\u30c0\u30e0\u4e88\u6e2c\n    auc_scores.append(auc)\n    print(f\"Fold {fold}: AUC = {auc:.4f}\")\n\nprint(f\"\\n\u2705 Mean CV AUC: {np.mean(auc_scores):.4f} \u00b1 {np.std(auc_scores):.4f}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}