{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":12726948,"sourceType":"datasetVersion","datasetId":8044304},{"sourceId":12762469,"sourceType":"datasetVersion","datasetId":8067935},{"sourceId":13016855,"sourceType":"datasetVersion","datasetId":8241242},{"sourceId":13038596,"sourceType":"datasetVersion","datasetId":8256201},{"sourceId":13044102,"sourceType":"datasetVersion","datasetId":8259759},{"sourceId":252850661,"sourceType":"kernelVersion"},{"sourceId":252853424,"sourceType":"kernelVersion"},{"sourceId":265160037,"sourceType":"kernelVersion"},{"sourceId":123002,"sourceType":"modelInstanceVersion","modelInstanceId":103527,"modelId":127747},{"sourceId":171496,"sourceType":"modelInstanceVersion","modelInstanceId":145960,"modelId":164048},{"sourceId":171638,"sourceType":"modelInstanceVersion","modelInstanceId":146086,"modelId":164048},{"sourceId":426330,"sourceType":"modelInstanceVersion","modelInstanceId":347541,"modelId":368803},{"sourceId":426333,"sourceType":"modelInstanceVersion","modelInstanceId":347543,"modelId":368803},{"sourceId":426337,"sourceType":"modelInstanceVersion","modelInstanceId":347547,"modelId":368803},{"sourceId":523492,"sourceType":"modelInstanceVersion","modelInstanceId":411182,"modelId":429004}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n### References\n\n*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch\n*   [https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference](https://www.kaggle.com/code/fuumin621/qwen2-5-lora-finetune-baseline-inference)\n*   https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b","metadata":{}},{"cell_type":"markdown","source":"### I want to say thanks to @neibyr for your interesting idea: [Retrieve by Qwen3Embedding](http://https://www.kaggle.com/code/neibyr/30-min-just-use-semantic-search-qwen3-emb-0-6b) ","metadata":{}},{"cell_type":"markdown","source":"This version changes the lr for training Qwen 3 0.5b. ","metadata":{}},{"cell_type":"code","source":"!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'trl==0.21.0' 'optimum==1.27.0' 'auto-gptq==0.7.1' 'bitsandbytes==0.46.1' 'deepspeed==0.17.4' 'logits-processor-zoo==0.2.1' 'vllm==0.10.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'triton==3.2.0'\n!uv pip install --system --no-index --find-links='/kaggle/input/jigsaw-packages2/whls/' 'clean-text'\n!uv pip install --system --no-index -U --no-deps --find-links='/kaggle/input/jigsaw-packages2/whls/' 'peft' 'accelerate' 'datasets'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:40:47.599292Z","iopub.execute_input":"2025-10-03T11:40:47.599942Z","iopub.status.idle":"2025-10-03T11:40:48.786208Z","shell.execute_reply.started":"2025-10-03T11:40:47.599917Z","shell.execute_reply":"2025-10-03T11:40:48.785398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Test time train Qwen 2.5 0.5b","metadata":{}},{"cell_type":"code","source":"%%writefile constants.py\nBASE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/0.5b-instruct-gptq-int4/1\"\nLORA_PATH = \"output/\"\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n\nPOSITIVE_ANSWER = \"Yes\"\nNEGATIVE_ANSWER = \"No\"\nCOMPLETE_PHRASE = \"Answer:\"\nBASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:40:48.787807Z","iopub.execute_input":"2025-10-03T11:40:48.788069Z","iopub.status.idle":"2025-10-03T11:40:48.79448Z","shell.execute_reply.started":"2025-10-03T11:40:48.788045Z","shell.execute_reply":"2025-10-03T11:40:48.793539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile rule_knowledge.py\n\nRULE_CANON_SHORT = {\n    \"public rule 0\": \"[ADVERTISING]\",\n    \"public rule 1\": \"[LEGAL ADVICE]\",\n    \"private rule 0\": \"[FINANCIAL ADVICE]\",\n    \"private rule 1\": \"[MEDICAL ADVICE]\",\n    \"private rule 2\": \"[ILLEGAL ACTIVITY]\",\n    \"private rule 3\": \"[SPOILERS]\",\n}\n\nKEYWORD_FALLBACK = [\n    (\"advertis|referral|promo|spam\", \"public rule 0\"),\n    (\"legal\", \"public rule 1\"),\n    (\"financ|invest|tax|career\", \"private rule 0\"),\n    (\"medical|diagnos|treat\", \"private rule 1\"),\n    (\"illegal|drug|violence|exploit|theft|crime\", \"private rule 2\"),\n    (\"spoiler\", \"private rule 3\"),\n]\n\nimport re\n\ndef canonicalize_rule(rule_str: str):\n    s = (rule_str or \"\").strip().lower()\n    # 直接一致\n    for k in RULE_CANON_SHORT:\n        if k in s:\n            return RULE_CANON_SHORT[k]\n    # キーワードでフォールバック\n    for pat, key in KEYWORD_FALLBACK:\n        if re.search(pat, s):\n            return RULE_CANON_SHORT[key]\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:40:48.795548Z","iopub.execute_input":"2025-10-03T11:40:48.795849Z","iopub.status.idle":"2025-10-03T11:40:48.813833Z","shell.execute_reply.started":"2025-10-03T11:40:48.795825Z","shell.execute_reply":"2025-10-03T11:40:48.813122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile text_cleaning.py\nimport regex as re  # pip install regex が必要\n\n# Unicode Emoji 全対応（ZWJ, スキントーンも含む）\n_EMOJI_RE = re.compile(r\"\\p{Emoji}+\")\n\n# Markdown装飾パターン\n_MD_PATTERNS = [\n    r\"\\*\\*(.*?)\\*\\*\",   # **bold**\n    r\"\\*(.*?)\\*\",       # *italic/bold*\n    r\"__(.*?)__\",       # __italic__\n    r\"_(.*?)_\",         # _italic_\n    r\"`(.*?)`\",         # `inline code`\n    r\"#+\\s+\",           # # Heading\n    r\">+\\s+\",           # > quote\n    r\"-{3,}\",           # --- hr\n]\n\n_MD_RE = re.compile(\"|\".join(_MD_PATTERNS), flags=re.MULTILINE)\n\ndef strip_emojis_kaomoji(text: str) -> str:\n    if not text:\n        return text\n    s = str(text)\n\n    # 絵文字削除\n    s = _EMOJI_RE.sub(\"\", s)\n\n    # Markdown装飾削除（内容は残す）\n    s = _MD_RE.sub(lambda m: m.group(1) if m.lastindex else \"\", s)\n\n    # 空白・改行の整形\n    s = re.sub(r\"[ \\t\\r\\f\\v]+\", \" \", s)\n    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n    return s\n\n\nif __name__ == \"__main__\":\n    sample = \"\"\"\n# **Huge SALE!!!**\nGet *FREE* stuff 👉👉 https://spam.com\n> Only today!!!\n😊🔥🚀\n\"\"\"\n    print(\"Before:\", sample)\n    print(\"After :\", strip_emojis_kaomoji(sample))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:41:01.072926Z","iopub.execute_input":"2025-10-03T11:41:01.073205Z","iopub.status.idle":"2025-10-03T11:41:01.07887Z","shell.execute_reply.started":"2025-10-03T11:41:01.073182Z","shell.execute_reply":"2025-10-03T11:41:01.078082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\nimport pandas as pd\nfrom datasets import Dataset\nfrom constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\nfrom rule_knowledge import canonicalize_rule\nfrom text_cleaning import strip_emojis_kaomoji as _strip\nimport random, numpy as np\nrandom.seed(42); np.random.seed(42)\n\ndef _sz(x):  # sanitize helper\n    return _strip(\"\" if pd.isna(x) else str(x))\n\ndef build_prompt(row):\n    rule_raw = _sz(row[\"rule\"])\n    body = _sz(row[\"body\"])\n    subreddit = _sz(row[\"subreddit\"])\n    pos_ex = _sz(row[\"positive_example\"])\n    neg_ex = _sz(row[\"negative_example\"])\n\n    canon = canonicalize_rule(rule_raw)\n    rule_block = f\"Rule: {rule_raw}\\n\"\n    if canon:\n        rule_block += f\"Canonical Definition: {canon}\\n\"\n\n    return f\"\"\"\n{BASE_PROMPT}\n\nSubreddit: r/{subreddit}\n{rule_block}\nExamples:\n1) {pos_ex}\n{COMPLETE_PHRASE} Yes\n\n2) {neg_ex}\n{COMPLETE_PHRASE} No\n\n---\nComment: {body}\n{COMPLETE_PHRASE}\"\"\"\n\ndef get_dataframe_to_train(data_path):\n    import numpy as np\n    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n\n    flatten = []\n\n    train_df = train_dataset[[\n        \"body\",\"rule\",\"subreddit\",\"rule_violation\",\n        \"positive_example_1\",\"positive_example_2\",\n        \"negative_example_1\",\"negative_example_2\"\n    ]].copy()\n\n    train_df[\"positive_example\"] = np.where(\n        np.random.rand(len(train_df)) < 0.5,\n        train_df[\"positive_example_1\"], train_df[\"positive_example_2\"]\n    )\n    train_df[\"negative_example\"] = np.where(\n        np.random.rand(len(train_df)) < 0.5,\n        train_df[\"negative_example_1\"], train_df[\"negative_example_2\"]\n    )\n    train_df.drop(columns=[\n        \"positive_example_1\",\"positive_example_2\",\n        \"negative_example_1\",\"negative_example_2\"\n    ], inplace=True)\n\n    # ここで主要テキスト列をクリーニング（学習データ）\n    for c in [\"body\",\"rule\",\"subreddit\",\"positive_example\",\"negative_example\"]:\n        train_df[c] = train_df[c].astype(str).map(_sz)\n\n    flatten.append(train_df)\n\n    # テスト例示からの flatten 拡張（ラベル付与）\n    for violation_type in [\"positive\", \"negative\"]:\n        for i in range(1, 2+1):\n            sub = test_dataset[[\n                \"rule\",\"subreddit\",\n                \"positive_example_1\",\"positive_example_2\",\n                \"negative_example_1\",\"negative_example_2\"\n            ]].copy()\n\n            if violation_type == \"positive\":\n                body_col = f\"positive_example_{i}\"\n                other_positive_col = f\"positive_example_{3-i}\"\n                sub[\"body\"] = sub[body_col]\n                sub[\"positive_example\"] = sub[other_positive_col]\n                sub[\"negative_example\"] = np.where(\n                    np.random.rand(len(sub)) < 0.5, sub[\"negative_example_1\"], sub[\"negative_example_2\"]\n                )\n                sub[\"rule_violation\"] = 1\n            else:\n                body_col = f\"negative_example_{i}\"\n                other_negative_col = f\"negative_example_{3-i}\"\n                sub[\"body\"] = sub[body_col]\n                sub[\"negative_example\"] = sub[other_negative_col]\n                sub[\"positive_example\"] = np.where(\n                    np.random.rand(len(sub)) < 0.5, sub[\"positive_example_1\"], sub[\"positive_example_2\"]\n                )\n                sub[\"rule_violation\"] = 0\n\n            sub.drop(columns=[\n                \"positive_example_1\",\"positive_example_2\",\"negative_example_1\",\"negative_example_2\"\n            ], inplace=True)\n\n            # 主要列をクリーニング（拡張データ）\n            for c in [\"body\",\"rule\",\"subreddit\",\"positive_example\",\"negative_example\"]:\n                sub[c] = sub[c].astype(str).map(_sz)\n\n            flatten.append(sub)\n\n    df = pd.concat(flatten, axis=0).drop_duplicates(ignore_index=True)\n    return df\n\ndef build_dataset(dataframe):\n    dataframe = dataframe.copy()\n    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n\n    columns = [\"prompt\"]\n    if \"rule_violation\" in dataframe:\n        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map({1: POSITIVE_ANSWER, 0: NEGATIVE_ANSWER})\n        columns.append(\"completion\")\n\n    dataset = Dataset.from_pandas(dataframe[columns])\n    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:41:03.147572Z","iopub.execute_input":"2025-10-03T11:41:03.147909Z","iopub.status.idle":"2025-10-03T11:41:03.155991Z","shell.execute_reply.started":"2025-10-03T11:41:03.147886Z","shell.execute_reply":"2025-10-03T11:41:03.15493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\nimport pandas as pd\n\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\nfrom tqdm.auto import tqdm\nfrom transformers.utils import is_torch_bf16_gpu_available\nfrom utils import build_dataset, get_dataframe_to_train\nfrom constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n\n\ndef main():\n    dataframe = get_dataframe_to_train(DATA_PATH)\n    train_dataset = build_dataset(dataframe)\n    \n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.1,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        task_type=\"CAUSAL_LM\",\n    )\n    \n    training_args = SFTConfig(\n        num_train_epochs=1,\n        \n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        \n        optim=\"paged_adamw_8bit\",\n        learning_rate=1e-4, #keep high, lora usually likes high. \n        weight_decay=0.01,\n        max_grad_norm=1.0,\n        \n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.03,\n        \n        bf16=is_torch_bf16_gpu_available(),\n        fp16=not is_torch_bf16_gpu_available(),\n        dataloader_pin_memory=True,\n        \n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n        save_strategy=\"no\",\n        report_to=\"none\",\n    \n        completion_only_loss=True,\n        packing=False,\n        remove_unused_columns=False,\n    )\n    \n    trainer = SFTTrainer(\n        BASE_MODEL_PATH,\n        args=training_args,\n        train_dataset=train_dataset,\n        peft_config=lora_config,\n    )\n    \n    trainer.train()\n    trainer.save_model(LORA_PATH)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:41:03.956569Z","iopub.execute_input":"2025-10-03T11:41:03.956894Z","iopub.status.idle":"2025-10-03T11:41:03.962312Z","shell.execute_reply.started":"2025-10-03T11:41:03.956873Z","shell.execute_reply":"2025-10-03T11:41:03.961493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile inference.py\nimport os\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nimport vllm\nimport torch\nimport pandas as pd\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom vllm.lora.request import LoRARequest\nfrom utils import build_dataset\nfrom constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\nimport random\nimport multiprocessing as mp\n\n\ndef run_inference_on_device(df_slice):\n    \"\"\"在当前进程可见的 GPU 上跑 vLLM 推理\"\"\"\n    llm = vllm.LLM(\n        BASE_MODEL_PATH,\n        quantization=\"gptq\",\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.98,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=2836,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=64,\n    )\n\n    tokenizer = llm.get_tokenizer()\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n\n    test_dataset = build_dataset(df_slice)\n    texts = test_dataset[\"prompt\"]\n\n    outputs = llm.generate(\n        texts,\n        vllm.SamplingParams(\n            skip_special_tokens=True,\n            max_tokens=1,\n            logits_processors=[mclp],\n            logprobs=2,\n        ),\n        use_tqdm=True,\n        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n    )\n\n    log_probs = [\n        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n        for out in outputs\n    ]\n    predictions = pd.DataFrame(log_probs)[[POSITIVE_ANSWER, NEGATIVE_ANSWER]]\n    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n    return predictions\n\n\ndef worker(device_id, df_slice, return_dict):\n    # 限制该进程只看到一张 GPU\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n\n    preds = run_inference_on_device(df_slice)\n    return_dict[device_id] = preds\n\n\ndef main():\n    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n\n    # 随机选择例子\n    test_dataframe[\"positive_example\"] = test_dataframe.apply(\n        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]),\n        axis=1\n    )\n    test_dataframe[\"negative_example\"] = test_dataframe.apply(\n        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]),\n        axis=1\n    )\n    test_dataframe = test_dataframe.drop(\n        columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n        errors=\"ignore\"\n    )\n\n    # 切分数据\n    mid = len(test_dataframe) // 2\n    df0 = test_dataframe.iloc[:mid].reset_index(drop=True)\n    df1 = test_dataframe.iloc[mid:].reset_index(drop=True)\n\n    manager = mp.Manager()\n    return_dict = manager.dict()\n\n    # 两个进程并行\n    p0 = mp.Process(target=worker, args=(0, df0, return_dict))\n    p1 = mp.Process(target=worker, args=(1, df1, return_dict))\n    p0.start()\n    p1.start()\n    p0.join()\n    p1.join()\n\n    # 合并结果\n    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\n\n    # 构建 submission\n    submission = predictions[[\"row_id\", POSITIVE_ANSWER]].rename(columns={POSITIVE_ANSWER: \"rule_violation\"})\n    rq = submission['rule_violation'].rank(method='average') / (len(submission) + 1)\n    submission['rule_violation'] = rq\n\n    submission.to_csv(\"submission_qwen.csv\", index=False)\n    print(\"✅ Saved submission_qwen.csv\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:41:06.296527Z","iopub.execute_input":"2025-10-03T11:41:06.297322Z","iopub.status.idle":"2025-10-03T11:41:06.303711Z","shell.execute_reply.started":"2025-10-03T11:41:06.297282Z","shell.execute_reply":"2025-10-03T11:41:06.302709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile accelerate_config.yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 4\n  gradient_clipping: 1.0\n  train_batch_size: 64\n  train_micro_batch_size_per_gpu: 4\n  \n  zero_stage: 2\n  offload_optimizer_device: none\n  offload_param_device: none\n  zero3_init_flag: false\n  \n  stage3_gather_16bit_weights_on_model_save: false\n  stage3_max_live_parameters: 1e8\n  stage3_max_reuse_distance: 1e8\n  stage3_prefetch_bucket_size: 5e7\n  stage3_param_persistence_threshold: 1e5\n  \n  zero_allow_untested_optimizer: true\n  zero_force_ds_cpu_optimizer: false\n  \n  fp16:\n    enabled: true\n    loss_scale: 0\n    initial_scale_power: 16\n    loss_scale_window: 1000\n    hysteresis: 2\n    min_loss_scale: 1\n  \ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\ndynamo_config:\n  dynamo_backend: INDUCTOR\n  dynamo_use_fullgraph: false\n  dynamo_use_dynamic: false\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:41:06.426647Z","iopub.execute_input":"2025-10-03T11:41:06.427224Z","iopub.status.idle":"2025-10-03T11:41:06.432049Z","shell.execute_reply.started":"2025-10-03T11:41:06.427203Z","shell.execute_reply":"2025-10-03T11:41:06.431378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch --config_file accelerate_config.yaml train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:41:09.647206Z","iopub.execute_input":"2025-10-03T11:41:09.647913Z","iopub.status.idle":"2025-10-03T11:50:51.660906Z","shell.execute_reply.started":"2025-10-03T11:41:09.647885Z","shell.execute_reply":"2025-10-03T11:50:51.65984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:50:51.66286Z","iopub.execute_input":"2025-10-03T11:50:51.663166Z","iopub.status.idle":"2025-10-03T11:52:08.854507Z","shell.execute_reply.started":"2025-10-03T11:50:51.663136Z","shell.execute_reply":"2025-10-03T11:52:08.853764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!head submission_qwen.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:52:08.855517Z","iopub.execute_input":"2025-10-03T11:52:08.855867Z","iopub.status.idle":"2025-10-03T11:52:08.976243Z","shell.execute_reply.started":"2025-10-03T11:52:08.855838Z","shell.execute_reply":"2025-10-03T11:52:08.975405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Qwen2.5 14B GPTQ Int4 Inference","metadata":{}},{"cell_type":"code","source":"# ! mkdir -p /tmp/src","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:52:08.978704Z","iopub.execute_input":"2025-10-03T11:52:08.979453Z","iopub.status.idle":"2025-10-03T11:52:08.983308Z","shell.execute_reply.started":"2025-10-03T11:52:08.979424Z","shell.execute_reply":"2025-10-03T11:52:08.982328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile infer_qwen.py\nimport os, math, pandas as pd, torch, vllm, numpy as np\nfrom typing import List, Dict\nfrom logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\nfrom vllm.lora.request import LoRARequest\nfrom rule_knowledge import canonicalize_rule\nfrom text_cleaning import strip_emojis_kaomoji as _strip\n\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\nMODEL_NAME = \"/kaggle/input/qwen2.5/transformers/14b-instruct-gptq-int4/1\"\nLORA_PATH  = \"/kaggle/input/lora_14b_gptq_1epoch_r32/keras/default/1\"\n\nGPU_MEM_UTIL = 0.90\nMAX_MODEL_LEN = 1036\nCHUNK_SIZE = 64\nMAX_TOK_BODY = 128\nMAX_TOK_EX   = 64\nMAX_TOK_RULE = 64\nSEED = 42\n\nPOS = \"Yes\"; NEG = \"No\"\n\ndef build_llm():\n    return vllm.LLM(\n        MODEL_NAME,\n        quantization=\"gptq\",\n        tensor_parallel_size=torch.cuda.device_count(),\n        gpu_memory_utilization=GPU_MEM_UTIL,\n        trust_remote_code=True,\n        dtype=\"half\",\n        enforce_eager=True,\n        max_model_len=MAX_MODEL_LEN,\n        disable_log_stats=True,\n        enable_prefix_caching=True,\n        enable_lora=True,\n        max_lora_rank=32,\n    )\n\ndef truncate_by_tokens(text: str, tokenizer, max_tokens: int) -> str:\n    if not text: return \"\"\n    ids = tokenizer.encode(text, add_special_tokens=False)\n    return text if len(ids) <= max_tokens else tokenizer.decode(ids[:max_tokens], skip_special_tokens=True)\n\nSYS_PROMPT = (\n    \"You are a strict rule-violation classifier for Reddit comments.\\n\"\n    \"Decide if the comment violates the given rule.\\n\"\n    \"- Use ONLY the rule (including the canonical definition) and the provided examples.\\n\"\n    \"- Ignore meme slang unless directly relevant to the rule.\\n\"\n    \"- If uncertain, answer 'No'.\\n\"\n    \"- Output EXACTLY one word: 'Yes' or 'No'.\"\n)\n\ndef build_prompt_rows(df: pd.DataFrame, tokenizer) -> List[str]:\n    prompts = []\n    apply_chat = lambda messages: tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n\n    for _, row in df.iterrows():\n        # まずクリーン\n        subreddit = _strip(row.get(\"subreddit\",\"\"))\n        rule_raw  = _strip(row.get(\"rule\",\"\"))\n        pos1 = _strip(row.get(\"positive_example_1\",\"\"))\n        pos2 = _strip(row.get(\"positive_example_2\",\"\"))\n        neg1 = _strip(row.get(\"negative_example_1\",\"\"))\n        neg2 = _strip(row.get(\"negative_example_2\",\"\"))\n        body = _strip(row.get(\"body\",\"\"))\n\n        canon = canonicalize_rule(rule_raw)\n        rule_block = f\"Rule: {rule_raw}\\n\"\n        if canon:\n            rule_block += f\"Canonical Definition: {canon}\\n\"\n\n        # トークン制限\n        rule_block = truncate_by_tokens(rule_block, tokenizer, MAX_TOK_RULE)\n        pos1 = truncate_by_tokens(pos1, tokenizer, MAX_TOK_EX)\n        pos2 = truncate_by_tokens(pos2, tokenizer, MAX_TOK_EX)\n        neg1 = truncate_by_tokens(neg1, tokenizer, MAX_TOK_EX)\n        neg2 = truncate_by_tokens(neg2, tokenizer, MAX_TOK_EX)\n        body = truncate_by_tokens(body, tokenizer, MAX_TOK_BODY)\n\n        user_text = (\n            f\"Subreddit: r/{subreddit}\\n\"\n            f\"{rule_block}\\n\"\n            f\"Examples of violation (Yes):\\n1) {pos1}\\n2) {pos2}\\n\\n\"\n            f\"Examples of non-violation (No):\\n3) {neg1}\\n4) {neg2}\\n\\n\"\n            f\"Target comment:\\n5) {body}\\n\"\n        )\n\n        prompt = apply_chat([\n            {\"role\":\"system\",\"content\":SYS_PROMPT},\n            {\"role\":\"user\",\"content\":user_text},\n        ]) + \"Answer:\"\n        prompts.append(prompt)\n    return prompts\n\ndef run_chunked_inference(llm, tokenizer, prompts: List[str]) -> List[Dict[str, float]]:\n    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POS, NEG])\n    all_lp = []\n    for s in range(0, len(prompts), CHUNK_SIZE):\n        batch = prompts[s:s+CHUNK_SIZE]\n        outs = llm.generate(\n            batch,\n            vllm.SamplingParams(skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=2),\n            use_tqdm=True,\n            lora_request=LoRARequest(\"default\", 1, LORA_PATH),\n        )\n        for out in outs:\n            lp_map = {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n            all_lp.append({POS: lp_map.get(POS, -1e9), NEG: lp_map.get(NEG, -1e9)})\n    return all_lp\n\ndef main():\n    torch.manual_seed(SEED); np.random.seed(SEED)\n    df = pd.read_csv(DATA_PATH)\n    llm = build_llm()\n    tokenizer = llm.get_tokenizer()\n\n    prompts = build_prompt_rows(df, tokenizer)\n    logprobs = run_chunked_inference(llm, tokenizer, prompts)\n\n    mat = pd.DataFrame(logprobs)[[POS, NEG]]\n    df = pd.concat([df, mat], axis=1)\n    df[\"logit_diff\"] = df[POS] - df[NEG]\n    df[\"rule_violation\"] = 1.0 / (1.0 + np.exp(-df[\"logit_diff\"]))\n    df[[\"row_id\",\"rule_violation\"]].to_csv(\"submission_qwen14b.csv\", index=False)\n    print(\"✅ Saved submission_qwen14b.csv\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:52:08.984421Z","iopub.execute_input":"2025-10-03T11:52:08.984716Z","iopub.status.idle":"2025-10-03T11:52:08.998564Z","shell.execute_reply.started":"2025-10-03T11:52:08.984688Z","shell.execute_reply":"2025-10-03T11:52:08.997723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %cd /tmp\n!python infer_qwen.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:52:08.999515Z","iopub.execute_input":"2025-10-03T11:52:08.999735Z","iopub.status.idle":"2025-10-03T11:55:41.896229Z","shell.execute_reply.started":"2025-10-03T11:52:08.999719Z","shell.execute_reply":"2025-10-03T11:55:41.895171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Qwen3 0.6b Embedding","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:55:41.897434Z","iopub.execute_input":"2025-10-03T11:55:41.897714Z","iopub.status.idle":"2025-10-03T11:55:42.262874Z","shell.execute_reply.started":"2025-10-03T11:55:41.89769Z","shell.execute_reply":"2025-10-03T11:55:42.262231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile constants.py\nEMBDEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\nMODEL_OUTPUT_PATH = '/kaggle/input/qwen3-8b-embedding'\nDATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules\"\n\n# https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/blob/main/config_sentence_transformers.json\nEMBEDDING_MODEL_QUERY = \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:\"\n\nCLEAN_TEXT = True\nTOP_K = 2000\nBATCH_SIZE = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:55:42.263597Z","iopub.execute_input":"2025-10-03T11:55:42.26402Z","iopub.status.idle":"2025-10-03T11:55:42.269274Z","shell.execute_reply.started":"2025-10-03T11:55:42.264Z","shell.execute_reply":"2025-10-03T11:55:42.26836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile utils.py\nimport pandas as pd\nimport torch.distributed as dist\nfrom datasets import Dataset\nfrom cleantext import clean\nfrom tqdm.auto import tqdm\nfrom text_cleaning import strip_emojis_kaomoji as _strip\nfrom constants import CLEAN_TEXT\n\ndef build_prompt(row):\n    return f\"\"\"r/{row[\"subreddit\"]}\\nComment: {row[\"body\"]}\"\"\"\n\ndef cleaner(text):\n    # clean-text でURL/EMAIL/PHONE等をマスク、Unicode整形\n    s = clean(\n        text,\n        fix_unicode=True,\n        to_ascii=True,           # 絵文字は基本ここで落ちる\n        lower=False,\n        no_line_breaks=False,\n        no_urls=True,\n        no_emails=True,\n        no_phone_numbers=True,\n        no_numbers=False,\n        no_digits=False,\n        no_currency_symbols=False,\n        no_punct=False,\n        replace_with_url=\"<URL>\",\n        replace_with_email=\"<EMAIL>\",\n        replace_with_phone_number=\"<PHONE>\",\n        lang=\"en\",\n    )\n    # ASCII系の顔文字や残留を追加で除去\n    s = _strip(s)\n    return s\n\ndef get_dataframe_to_train(data_path):\n    train_dataset = pd.read_csv(f\"{data_path}/train.csv\")\n    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.6, random_state=42).reset_index(drop=True)\n\n    flatten = []\n    flatten.append(train_dataset[[\"body\",\"rule\",\"subreddit\",\"rule_violation\"]])\n\n    for violation_type in [\"positive\",\"negative\"]:\n        for i in range(1,3):\n            sub = test_dataset[[f\"{violation_type}_example_{i}\",\"rule\",\"subreddit\"]].copy()\n            sub = sub.rename(columns={f\"{violation_type}_example_{i}\":\"body\"})\n            sub[\"rule_violation\"] = 1 if violation_type == \"positive\" else 0\n            flatten.append(sub)\n\n    df = pd.concat(flatten, axis=0).drop_duplicates(ignore_index=True)\n    return df\n\ndef prepare_dataframe(dataframe):\n    dataframe = dataframe.copy()\n    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n    if CLEAN_TEXT:\n        tqdm.pandas(desc=\"cleaner\")\n        dataframe[\"prompt\"] = dataframe[\"prompt\"].progress_apply(cleaner)\n\n    if \"rule_violation\" in dataframe.columns:\n        dataframe[\"rule_violation\"] = dataframe[\"rule_violation\"].map({1: 1, 0: -1})\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:55:42.270311Z","iopub.execute_input":"2025-10-03T11:55:42.270569Z","iopub.status.idle":"2025-10-03T11:55:42.282914Z","shell.execute_reply.started":"2025-10-03T11:55:42.270546Z","shell.execute_reply":"2025-10-03T11:55:42.282229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile semantic.py\nimport pandas as pd\nfrom transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import semantic_search, dot_score\nfrom tqdm.auto import tqdm\nfrom peft import PeftModel, PeftConfig\n\n\nfrom utils import get_dataframe_to_train, prepare_dataframe\nfrom constants import DATA_PATH, EMBDEDDING_MODEL_PATH, EMBEDDING_MODEL_QUERY, TOP_K, BATCH_SIZE, MODEL_OUTPUT_PATH\n\n\n\ndef get_scores(test_dataframe):\n    corpus_dataframe = get_dataframe_to_train(DATA_PATH)\n    corpus_dataframe = prepare_dataframe(corpus_dataframe)\n    \n    # Load base model\n    model = AutoModelForCausalLM.from_pretrained(EMBDEDDING_MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(EMBDEDDING_MODEL_PATH)\n    \n    # Load adapter configuration and model\n    adapter_config = PeftConfig.from_pretrained(MODEL_OUTPUT_PATH)\n    lora_model = PeftModel.from_pretrained(model, MODEL_OUTPUT_PATH, config=adapter_config)\n    merged_model = lora_model.merge_and_unload()\n    tokenizer.save_pretrained(\"Qwen3Emb_Finetuned\")\n    merged_model.save_pretrained(\"Qwen3Emb_Finetuned\")\n\n    # 4. Tạo lại SentenceTransformer từ encoder đã merge\n    embedding_model = SentenceTransformer(model_name_or_path=\"Qwen3Emb_Finetuned\", device=\"cuda\")\n\n    print('Done loading model!')\n\n    result = []\n    for rule in tqdm(test_dataframe[\"rule\"].unique(), desc=f\"Generate scores for each rule\"):\n        test_dataframe_part = test_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n        corpus_dataframe_part = corpus_dataframe.query(\"rule == @rule\").reset_index(drop=True)\n        corpus_dataframe_part = corpus_dataframe_part.reset_index(names=\"row_id\")\n        \n        query_embeddings = embedding_model.encode(\n            sentences=test_dataframe_part[\"prompt\"].tolist(),\n            prompt=EMBEDDING_MODEL_QUERY,\n            batch_size=BATCH_SIZE,\n            show_progress_bar=True,\n            convert_to_tensor=True,\n            device=\"cuda\",\n            normalize_embeddings=True,\n        )\n        document_embeddings = embedding_model.encode(\n            sentences=corpus_dataframe_part[\"prompt\"].tolist(),\n            batch_size=BATCH_SIZE,\n            show_progress_bar=True,\n            convert_to_tensor=True,\n            device=\"cuda\",\n            normalize_embeddings=True,\n        )\n        test_dataframe_part[\"semantic\"] = semantic_search(\n            query_embeddings,\n            document_embeddings,\n            top_k=TOP_K,\n            score_function=dot_score,\n        )\n        def get_score(semantic):\n            semantic = pd.DataFrame(semantic)\n            semantic = semantic.merge(\n                corpus_dataframe_part[[\"row_id\", \"rule_violation\"]],\n                how=\"left\",\n                left_on=\"corpus_id\",\n                right_on=\"row_id\",\n            )\n            semantic[\"score\"] = semantic[\"score\"]*semantic[\"rule_violation\"]\n            return semantic[\"score\"].sum()\n            \n        tqdm.pandas(desc=f\"Add label for {rule=}\")\n        test_dataframe_part[\"rule_violation\"] = test_dataframe_part[\"semantic\"].progress_apply(get_score)\n        result.append(test_dataframe_part[[\"row_id\", \"rule_violation\"]].copy())\n        \n    submission = pd.concat(result, axis=0)\n    return submission\n\n\ndef generate_submission():\n    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n    test_dataframe = prepare_dataframe(test_dataframe)\n    \n    submission = get_scores(test_dataframe)\n    submission = test_dataframe[[\"row_id\"]].merge(submission, on=\"row_id\", how=\"left\")\n    submission.to_csv(\"submission_qwen3.csv\", index=False)\n\n\nif __name__ == \"__main__\":\n    generate_submission()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:55:42.284925Z","iopub.execute_input":"2025-10-03T11:55:42.28518Z","iopub.status.idle":"2025-10-03T11:55:42.298878Z","shell.execute_reply.started":"2025-10-03T11:55:42.285163Z","shell.execute_reply":"2025-10-03T11:55:42.298253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python semantic.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T11:55:42.29943Z","iopub.execute_input":"2025-10-03T11:55:42.299671Z","iopub.status.idle":"2025-10-03T12:00:56.212473Z","shell.execute_reply.started":"2025-10-03T11:55:42.299656Z","shell.execute_reply":"2025-10-03T12:00:56.211689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. ENSEMBLE RESULT","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nq = pd.read_csv('submission_qwen.csv')\nl = pd.read_csv('submission_qwen3.csv')\nm = pd.read_csv('submission_qwen14b.csv')\n\n\nrq = q['rule_violation'].rank(method='average') / (len(q)+1)\nrl = l['rule_violation'].rank(method='average') / (len(l)+1)\nrm = m['rule_violation'].rank(method='average') / (len(m)+1)\n\n\nblend = 0.5*rq + 0.3*rl + 0.2*rm   # or tune the rank-weights with a tiny grid using OOF\nq['rule_violation'] = blend\nq.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T12:00:56.213636Z","iopub.execute_input":"2025-10-03T12:00:56.214585Z","iopub.status.idle":"2025-10-03T12:00:56.238502Z","shell.execute_reply.started":"2025-10-03T12:00:56.214544Z","shell.execute_reply":"2025-10-03T12:00:56.237971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\npd.read_csv('/kaggle/working/submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T12:00:56.23928Z","iopub.execute_input":"2025-10-03T12:00:56.23959Z","iopub.status.idle":"2025-10-03T12:00:56.262733Z","shell.execute_reply.started":"2025-10-03T12:00:56.239563Z","shell.execute_reply":"2025-10-03T12:00:56.262076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom utils import get_dataframe_to_train\nfrom constants import DATA_PATH\n\n# --- 学習データの取得 ---\ntrain_df = get_dataframe_to_train(DATA_PATH)\n\n# --- ラベルと特徴の準備 ---\ny_true = train_df[\"rule_violation\"]\nX_dummy = np.zeros(len(train_df))  # 特徴量を使わないダミー分割\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nauc_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_dummy, y_true), 1):\n    y_train, y_val = y_true.iloc[train_idx], y_true.iloc[val_idx]\n    auc = roc_auc_score(y_val, np.random.rand(len(y_val)))  # 仮にランダム予測\n    auc_scores.append(auc)\n    print(f\"Fold {fold}: AUC = {auc:.4f}\")\n\nprint(f\"\\n✅ Mean CV AUC: {np.mean(auc_scores):.4f} ± {np.std(auc_scores):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}