# Gemini Project Context: Jigsaw Agile Community Rules Classification

## Project Overview

This project is set up for the [Jigsaw Agile Community Rules Kaggle competition](https://www.kaggle.com/competitions/jigsaw-agile-community-rules/overview). The goal is to classify whether a given Reddit comment violates a specific community rule.

The overall strategy involves fine-tuning multiple large language models (LLMs) like Qwen using techniques such as LoRA (Low-Rank Adaptation), running inference to get predictions, and then ensembling the results to produce a final submission. The project also utilizes semantic search as part of the prediction process.

**Key Technologies:**
*   Python
*   Pandas
*   PyTorch
*   Hugging Face Suite (Transformers, PEFT, TRL, Datasets)
*   vLLM
*   Sentence-Transformers

## Building and Running

The primary workflow is orchestrated from the `notebooks/notebook.ipynb` Jupyter notebook. This notebook is designed to run in a Kaggle environment and contains the logic for data processing, model training, and inference.

**Execution Flow:**

1.  **Data Setup:** The competition data is expected to be in the `data/` directory. The scripts currently reference Kaggle's default data path (`/kaggle/input/jigsaw-agile-community-rules/`). For local execution, you will need to download the data and place it in the `data/` folder.
2.  **Code Generation:** The main notebook (`notebook.ipynb`) contains cells that write several Python scripts (`train.py`, `inference.py`, `utils.py`, etc.) and configuration files.
3.  **Model Training:** The training process is launched using Hugging Face Accelerate. The command can be found in the notebook:
    ```bash
    !accelerate launch --config_file accelerate_config.yaml train.py
    ```
4.  **Inference:** Predictions are generated by running the inference scripts:
    ```bash
    !python inference.py
    ```
5.  **Submission:** The final `submission.csv` is created by blending the outputs from different models.

**Dependencies:**

A `requirements.txt` file is not yet present. Based on the code, the following libraries are required:

```
# TODO: Create a requirements.txt file
pandas
datasets
trl
peft
transformers
vllm
sentence-transformers
cleantext
torch
scikit-learn
```

## Development Conventions

The project is organized into the following directories:

*   `data/`: Contains the raw competition data (e.g., `train.csv`, `test.csv`).
*   `notebooks/`: For exploratory data analysis (EDA) and the main experiment orchestration notebook (`notebook.ipynb`).
*   `src/`: Holds modular Python code for data processing (`utils.py`), model training (`train.py`), and inference (`inference.py`). The `extracted_code.py` file is a raw dump from the notebook and should be split into the individual files it represents for better organization.
*   `models/`: For storing trained model artifacts, such as LoRA adapters.
*   `submissions/`: The target directory for outputting `submission.csv` files.
